<!DOCTYPE html>
<!-- _layouts/distill.html -->
<html>
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>CS-330 Lecture 1: Multi-Task Learning | Lars C.P.M. Quaedvlieg</title>
    <meta name="author" content="Lars C.P.M. Quaedvlieg" />
    <meta name="description" content="This is the first lecture of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the key design decisions when building multi-task learning systems." />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/default.css" media="" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/blog/2024/cs330-stanford-mtl/">
    
    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "CS-330 Lecture 1: Multi-Task Learning",
      "description": "This is the first lecture of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the key design decisions when building multi-task learning systems.",
      "published": "March 2, 2024",
      "authors": [
        {
          "author": "Lars C.P.M. Quaedvlieg",
          "authorURL": "https://lars-quaedvlieg.github.io/",
          "affiliations": [
            {
              "name": "EPFL",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Lars C.P.M. Quaedvlieg</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">Curriculum Vitae</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>CS-330 Lecture 1: Multi-Task Learning</h1>
        <p>This is the first lecture of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the key design decisions when building multi-task learning systems.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#problem-statement">Problem statement</a></div>
            <div><a href="#models-objectives-optimization">Models, objectives, optimization</a></div>
            <ul>
              <li><a href="#model">Model</a></li>
              <li><a href="#objectives">Objectives</a></li>
              <li><a href="#optimization">Optimization</a></li>
              
            </ul>
<div><a href="#challenges">Challenges</a></div>
            <div><a href="#case-study-of-real-world-multi-task-learning">Case study of real-world multi-task learning</a></div>
            <div><a href="#references">References</a></div>
            
          </nav>
        </d-contents>

        <p>The goal of this lecture is to understand the key design decisions when building multi-task learning systems. Since I am
still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or 
whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/" target="_blank" rel="noopener noreferrer">LinkedIn</a>. ☺</p>

<p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_multitask_transfer_2023.pdf" target="_blank" rel="noopener noreferrer">here</a>.</p>

<h2 id="problem-statement">Problem statement</h2>

<p>We will first establish some notation that will be used throughout the course. Let’s first introduce the single-task 
supervised learning problem.</p>

\[\min_\theta \mathcal{L}(\theta, \mathcal{D}), \quad \text{s.t.} \quad \mathcal{D} = \{(x,y)_k\}\;.\]

<p>Here, $\mathcal{L}$ is the loss function, $\theta$ are the model parameters and $\mathcal{D}$ is the dataset. A typical 
example of a loss function would be the negative log-likelihood function $\mathcal{L}(\theta, \mathcal{D}) = - \mathbb{E}\left[\log f_\theta(y\vert x)\right]$.</p>

<p>We can formally define a <strong>task</strong> as follows<strong>:</strong></p>

\[\mathcal{T}_i := \{p_i(x), p_i(y\vert x), \mathcal{L}_i\}\;.\]

<p>Here, $p_i(x)$ is the input data distribution, $p_i(y\vert x)$ is the distribution of the target variable(s), and 
$\mathcal{L}_i$ is a task-specific loss function (can of course be the same for different tasks). The corresponding 
task datasets are $\mathcal{D}_i^\mathrm{tr} := \mathcal{D}_i$ and $\mathcal{D}_i^\mathrm{test}$.</p>

<p>Some examples of tasks:</p>

<ul>
   <li>Multi-task classification ($\mathcal{L_i}$ the same for each task)</li>
   <ul>
      <li>Per-language handwriting recognition.</li>
      <li>Personalized spam filter.</li>
   </ul>
   <li>Multi-label learning ($\mathcal{L_i}$ and $p_i(x)$ the same for each task)</li>
   <ul>
      <li>Face attribute recognition.</li>
      <li>Scene understanding.
      <div class="col-sm-5 mt-3 mt-md-0">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/weighted_mtl_objective.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>
      </li>
   </ul>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/8.jpg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/10.jpg" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<p>It is important to realize that $\mathcal{L}_i$ might change across tasks, for example when mixing discrete from
continuous data or if there are multiple metrics that you care about.</p>

<hr>

<h2 id="models-objectives-optimization">Models, objectives, optimization</h2>

<p>One way of helping a model identify different tasks would be to condition the model function by a task descriptor 
$z_i$: $f_\theta(y\vert x, z_i)$. This could be anything ranging from user features, language descriptions, or formal 
task specifications. The next subsections will focus on how to condition the model, which objective should be used, and 
how the objective should be optimized.</p>

<h3 id="model">Model</h3>

<div>
<figure class="figure col-sm-8 float-right">
    <img src="/assets/img/blog/cs330/2/mult_gating.png" class="img-fluid" alt="Alt text.">
    <figcaption class="figure-caption text-center">Network architecture for task-specific independent subnetworks.</figcaption>
</figure>

<p>Let’s first think about how we can condition on the task in order to share <b>as little information</b> as possible. The
answer to this is simple: you can create a function that uses multiplicative gating with a one-hot encoding of the task
. The model function would be $f_\theta(y \vert x, z_i) = \sum_j \mathbb{1}(z_i=j)f_{\theta_i}(x)$. This results in
independent training with a single network per tasks; there are no shared parameters. This can be seen in the figure above.</p>
</div>

<p>On the other extreme, you could simply concatenate $z_i$ with the input and/or activations in the model. In this case, 
all parameters are shared (except the ones directly following $z_i$, in case it is one-hot).</p>

<p>This give rise to a question: can you phrase the multi-task learning objective parameters $\theta = \theta_\mathrm{sh} 
\cup \theta_i$, where $\theta_\mathrm{sh}$ are shared parameters and $\theta_i$ are task-specific parameters? Our 
objective function becomes the following:</p>

\[\min_{\theta_\mathrm{sh}, \theta_1, \dots, \theta_T} \sum_{i=1}^T \mathcal{L}_i(\theta_\mathrm{sh} \cup \theta_i, \mathcal{D}_i)\;.\]

<p>In this case, choosing how to condition on $z_i$ is equivalent to choosing how and where to share model parameters. We 
will now look into some basic ways to condition a model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/concat_cond.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Concatenation-based conditioning.</figcaption>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/additive_cond.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Additive conditioning.</figcaption>
    </div>
</div>

<p><br>Can you see why additive conditioning in this way is equivalent to concatenation-based conditioning? Hint: think about 
how matrix multiplication splits the parameters when concatenating<d-footnote>You can find the solution to this question in the <a href="https://cs330.stanford.edu/materials/cs330_multitask_transfer_2023.pdf" target="_blank" rel="noopener noreferrer">lecture slides</a> (slide 13).</d-footnote>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/multi_head.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Multi-head architecture conditioning.</figcaption>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/mult_cond.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Multiplicative conditioning.</figcaption>
    </div>
</div>
<p><br></p>

<p>One benefit of multiplicative conditioning is that you have this multiplicative gating, allowing more expressiveness 
per layer. It generalizes independent networks and independent heads.</p>

<p>There are more complex conditioning techniques, and a lot of research has gone into this topic, such as Cross-Stitch Networks <d-cite key="misra2016cross"></d-cite>, 
Multi-Task Attention Network <d-cite key="liu2019end"></d-cite>, Deep Relation Networks <d-cite key="dai2017detecting"></d-cite>, 
Perceiver IO <d-cite key="jaegle2021perceiver"></d-cite>, and more.</p>

<p>Unfortunately, these design choices are <strong>problem dependent</strong>, largely guided by <strong>intuition</strong> or <strong>knowledge</strong> about 
the problem, and currently more of an <strong>art</strong> than a science.</p>

<h3 id="objectives">Objectives</h3>

<p>We already saw a previous example of a multi-task objective function. Let’s start with the vanilla multi-task learning 
(MTL) objective: $\min_\theta \sum_{i=1}^T \mathcal{L}_i(\theta, \mathcal{D_i})$. Let’s now show some other ways to 
construct multi-task objective functions.</p>

<ol>
  <li>
    <p>Weighted multi-task learning (manually based on priority or dynamically adjust weights throughout training):</p>

\[\min_\theta \sum_{i=1}^T w_i \mathcal{L}_i(\theta, \mathcal{D_i})\;.\]
  </li>
  <li>
    <p>Minimax multi-task learning to optimize for the worst-case task loss (useful in robustness or fairness):</p>

\[\min_\theta \max_i \mathcal{L}_i(\theta, \mathcal{D_i})\;.\]
  </li>
  <li>
    <p>You can use various <strong>heuristics</strong> to construct your objective function. One example is to encourage gradients to have similar magnitudes across tasks.</p>
  </li>
</ol>

<h3 id="optimization">Optimization</h3>

<p>For the vanilla MTL objective, a basic training approach follows the following steps:</p>

<ol>
  <li>Sample mini-batch of tasks $\mathcal{B} = {\mathcal{T}_i}$.</li>
  <li>Sample mini-batch of datapoints for each task $\mathcal{D}^b_i \sim \mathcal{D}_i$.</li>
  <li>Compute mini-batch loss $\hat{\mathcal{L}}(\theta, \mathcal{B}) = \sum_{\mathcal{T}_k \in \mathcal{B}} \mathcal{L}_k(\theta, \mathcal{D}_k^b)$.</li>
  <li>Backpropagate the loss to compute $\nabla_\theta \hat{\mathcal{L}}$.</li>
  <li>Perform a step of gradient descent with some optimizer.</li>
  <li>Repeat from step 1.</li>
</ol>

<p>This process ensures that tasks are sampled uniformly, regardless of data quantities. However, it is important to ensure 
that the task labels, and the loss function, are on the same scale.</p>

<hr>

<h2 id="challenges">Challenges</h2>

<p>There are multiple challenges that come with multi-task learning.</p>

<ol>
  <li>
    <p><strong>Negative transfer</strong>: Sometimes independent subnetworks work better than parameter sharing. This could be due to <strong>optimization challenges</strong> (cross-task interference or tasks learning at different rates), or <strong>limited representational capacity</strong> (multi-task networks often need to be <em>much larger</em> than their single-task counterparts).</p>

    <p>In the case of negative transfer, you should share less across tasks. You can also add a regularization term to the objective function, to allow <em>soft parameter sharing</em>:</p>

\[\min_{\theta_\mathrm{sh}, \theta_1, \dots, \theta_T} \sum_{i=1}^T \mathcal{L}_i(\theta_\mathrm{sh} \cup \theta_i, \mathcal{D}_i) + \lambda \sum_{i^\prime = 1}^T \left\Vert \theta_i - \theta_i^\prime \right\Vert\;.\]

    <p>This allows for more fluid degrees of parameters sharing. However, it does add another set of hyperparameters, and it more memory intensive.</p>
  </li>
  <li>
<strong>Overfitting</strong>: You might not be sharing enough parameters. Since multi-task learning is equivalent to a form of regularization, the solution could be to share more parameters.</li>
  <li>
<strong>Having many tasks</strong>: You might wonder how to train all tasks together and which ones will be complementary. Unfortunately, no closed-form solution exists for measuring task similarity. Nevertheless, there are ways to approximate it from one training run <d-cite key="fifty2021efficiently"></d-cite> <d-cite key="xie2024doremi"></d-cite>.</li>
</ol>

<hr>

<h2 id="case-study-of-real-world-multi-task-learning">Case study of real-world multi-task learning</h2>

<p>In this case study, we will discuss the paper “Recommending What Video to Watch Next: A Multitask Ranking System” <d-cite key="zhao2019recommending"></d-cite>. They 
introduce a large scale multi-objective ranking system for recommending what video to watch next on an industrial video 
sharing platform. The system faces many real-world challenges, including the presence of multiple competing ranking 
objectives, as well as implicit selection biases in user feedback.</p>

<p>The framework is constructed as follows:</p>

<ul>
  <li>
<strong>Inputs</strong>: What the user is currently watching (query video) and user features</li>
</ul>

<p>The procedure is the following:</p>

<ol>
  <li>Generate a few hundred of <strong>candidate videos</strong> (by pooling videos from multiple candidate generation algorithms such as matching topics of the query video, videos frequently watched with the query video, and others).</li>
  <li>
<strong>Rank</strong> the candidates.</li>
  <li>
<strong>Serve</strong> the top ranking videos to the user.</li>
</ol>

<p>The central topic of this paper is the ranking system. The authors decide that the inputs to the ranking model are the 
<strong>query video</strong>, <strong>candidate video</strong>, and <strong>context features</strong>. The model attempts to output a weighted combination of 
<strong>engagement</strong> and <strong>satisfaction</strong> predictions, which results in a ranking score. The score weights are manually tuned.</p>

<div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/2/expert_model.png" class="img-fluid" alt="Alt text.">
    <figcaption class="figure-caption text-center">Multi-gate Mixture-of-Expert architecture.</figcaption>
</figure>

<p>On choice for the model architecture is a “shared-bottom model”, which has some shared bottom layers which split into
separate heads for each task. However, this will harm learning when the correlation between tasks is low. Instead, they
opt for a form of soft-parameter sharing that they call <b>Multi-gate Mixture-of-Experts</b> (MMoE). As you can see in the
figure, this architecture allows different parts of the network to “specialize” in certain tasks as experts. For each
task, an attention-like score is computed that decides which combination of experts should be used.</p>
</div>

<p>Formally, let’s call the expert networks $f_i(x)$. We then decide which expert to use for input $x$ and task $k$ by 
computing $g^k(x) = \mathrm{softmax}(W_{g^k}x)$. The features are then computed from the selected experts as 
$f_k(x) = \sum_{i=1}^n g_{(i)}^k(x)f_i(x)$. The output can finally be denoted by $y_k = h^k(f^k(x))$.</p>

<p>In the paper, they trained them model in temporal order, running training continuously to consume newly arriving data. 
They perform online A/B testing in comparison to the production system based on some live metrics, and stress that
model <strong>computational efficiency matters</strong>.</p>

<div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/2/paper_results.png" class="img-fluid" alt="Alt text.">
    <figcaption class="figure-caption text-center">Results from different model configurations.</figcaption>
</figure>

<p>From the results, you can see that this sort of architecture definitely helps. Furthermore, they found that there was a 
20% change of <b>gating polarization</b> during distributed training. This means that not all experts are utilized equally
and there is a bias to some expert(s). They utilized drop-out on the experts to counteract this problem.</p>
</div>

<hr>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Lars C.P.M. Quaedvlieg. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <d-bibliography src="/assets/bibliography/blog/cs330/2024-03-02-mtl.bib"></d-bibliography>
    
    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
