<!DOCTYPE html>
<!-- _layouts/distill.html -->
<html>
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>CS-330 Lecture 3: Black-Box Meta-Learning & In-Context Learning | Lars C.P.M. Quaedvlieg</title>
    <meta name="author" content="Lars C.P.M. Quaedvlieg" />
    <meta name="description" content="This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to implement black-box meta-learning techniques. We will also talk about a case study of GPT-3!" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/default.css" media="" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/blog/2024/cs330-stanford-bbml-icl/">
    
    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "CS-330 Lecture 3: Black-Box Meta-Learning & In-Context Learning",
      "description": "This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to implement black-box meta-learning techniques. We will also talk about a case study of GPT-3!",
      "published": "March 3, 2024",
      "authors": [
        {
          "author": "Lars C.P.M. Quaedvlieg",
          "authorURL": "https://lars-quaedvlieg.github.io/",
          "affiliations": [
            {
              "name": "EPFL",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Lars C.P.M. Quaedvlieg</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">Curriculum Vitae</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>CS-330 Lecture 3: Black-Box Meta-Learning &amp; In-Context Learning</h1>
        <p>This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to implement black-box meta-learning techniques. We will also talk about a case study of GPT-3!</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#black-box-adaptation-approaches">Black-box adaptation approaches</a></div>
            <ul>
              <li><a href="#a-more-scalable-architecture">A more scalable architecture</a></li>
              <li><a href="#black-box-adaptation-architectures">Black-box adaptation architectures</a></li>
              
            </ul>
<div><a href="#case-study-of-gpt-3">Case study of GPT-3</a></div>
            
          </nav>
        </d-contents>

        <p>The goal of this lecture is to learn how to <strong>implement black-box meta-learning</strong> techniques. We will also talk about a
<strong>case study of GPT-3</strong>! If you missed the previous lecture, which was about transfer learning by fine-tuning and meta
learning, you can head over <a href="/blog/2024/cs330-stanford-tl-ml/">here</a> to view it.</p>

<p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow 
of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/" target="_blank" rel="noopener noreferrer">LinkedIn</a>. ☺</p>

<p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_metalearning_bbox_2023.pdf" target="_blank" rel="noopener noreferrer">here</a>.</p>

<h2 id="black-box-adaptation-approaches">Black-box adaptation approaches</h2>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/omniglot.png" class="img-fluid" alt="Alt text.">
    <figcaption class="figure-caption text-center">Example of the Omniglot dataset.</figcaption>
</figure>

<p>The content of this section will build on the general recipe for meta-learning problems that we saw in the previous 
lecture. In order to explain it, we will use the example of the Omniglot dataset <d-cite key="lake2019omniglot"></d-cite>, which is a dataset of 1,623 
characters from 50 different alphabets. In this problem, every alphabet would refer to a different task. In our example,
we will do 3-way 1-shot learning, meaning that our sampled datasets consist of 3 classes with 1 example per class at 
every step. One iteration of the black-box meta-training process then has the following steps:</p>

<ol>
  <li>Sample task $\mathcal{T}_i$ or a mini-batch of tasks. In our case, this would correspond to generating the language(s).</li>
  <li>From the selected language(s), we sample disjoint datasets $\mathcal{D}_i^\mathrm{tr}$ and $\mathcal{D}_i^\mathrm{test}$ from $\mathcal{D}_i$. In our example, this will be a disjoint dataset with 3 samples of characters for every language alphabet.</li>
</ol>

<div>
<figure class="figure col-sm-5 6 float-right">
    <img src="/assets/img/blog/cs330/4/param_lstm.png" class="img-fluid" alt="Alt text.">
    <figcaption class="figure-caption text-center">Basic model architecture for black-box meta learning.</figcaption>
</figure>

<p>Now that we have these datasets, our goal is to train a neural network to represent $\phi_i = f_\theta(\mathcal{D}_i^\mathrm{tr})$. 
After computing these task parameters given a sampled training dataset, we can predict the test targets with $y^\mathrm{ts} = g_{\phi_i}(x^\mathrm{ts})$. 
An example of how such a model could work, is depicted in the figure above. Here, we are using a sequence model for 
$f_\theta$, which generates the parameters $\phi_i$. However, you can use all your fancy architectures that can handle 
a varying number of input sample. This is necessary due to varying dataset lengths.</p>
</div>

<p>After computing $y^\mathrm{ts}$, we can do backpropagation of the loss that is generated with the this test dataset. 
The full optimization objective is shown in the equation below:</p>

\[\min_\theta \sum_{\mathcal{T}_i} \sum_{(x,y) \sim \mathcal{D}^\mathrm{test}_i} - \log g_{\phi_i}(y\vert x) = \min_\theta \sum_{\mathcal{T}_i}\mathcal{L}(f_\theta(\mathcal{D}^\mathrm{tr}_i), \mathcal{D}^\mathrm{test}_i)\;.\]

<p>Notice that we are optimizing the parameters $\theta$. The task-specific parameters $\phi_i$ are generated by 
$f_\theta(\mathcal{D}_i^\mathrm{tr})$, and so they are not updated. Also note that the loss is calculated with 
respect to the sampled <strong>test dataset</strong>! This is no problem, since it makes sense to evaluate on new tasks for meta 
learning.</p>

<p>Now that you understand the architecture, we can write down the last two steps of the meta-training process:</p>

<ol>
  <li>Compute $\phi_i \leftarrow f_\theta(\mathcal{D}_i^\mathrm{tr})$.</li>
  <li>Update $\theta$ using $\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$.</li>
</ol>

<h3 id="a-more-scalable-architecture">A more scalable architecture</h3>

<p>However, we run into an issue. How do we let the model $f_\theta$ output another model’s parameters $\phi_i$? Not only 
can this be quite tricky to do, it also does not scale to larger parameter vectors $\phi_i$! Can you think of an 
alternative way of going this?</p>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/better_model.png" class="img-fluid" alt="Alt text.">
    <figcaption class="figure-caption text-center">More scalable architecture for black-box meta-learning.</figcaption>
</figure>

<p>Instead of letting $f_\theta$ output $\phi_i$, we instead output a hidden state $h_i$, which is a low-dimensional 
vector that is supposed to represent contextual task information from the training dataset. If you recall the different
ways of conditioning that we saw for multi-task learning, you can see that we can train a model end-to-end by 
conditioning as $y^\mathrm{ts} = g_{\phi}(x^\mathrm{ts} \vert h_i)$. Now, notice that we have a general set of 
parameters $\phi$ for $g$; it does not need to be task-specific anymore, since we are already conditioning on task
information. In the figure above, $\theta$ are the parameters of the sequence model, and $\phi$ are the parameters of 
the convolutional network.</p>

<p>❗One problem that sometimes occurs with this architecture, is that the model learns to <strong>ignore conditioning on $h_i$.</strong>
In that case, it is essentially just learning to memorize, and not using the training dataset. In order to avoid that,
you can randomize the numerical label assignment to the target variables when sampling the datasets $\mathcal{D}^{tr}_i$
and $\mathcal{D}^{test}_i$. If the numerical label is different each time, it cannot just memorize the sample from the 
testing set.</p>

<h3 id="black-box-adaptation-architectures">Black-box adaptation architectures</h3>

<p>The architecture that we just presented was more-or-less first proposed on the Omniglot dataset at ICML in 2016 <d-cite key="santoro2016meta"></d-cite>.
It used LSTMs with Neural Turing Machines (which are not used anymore nowadays). Since then, a lot of new architectures 
have been proposed.</p>

<p>At ICML 2018, an architecture called the DeepSet architecture <d-cite key="garnelo2018conditional"></d-cite> was published. The idea is to pass all your dataset 
samples through a feedforward neural network to get an embedding of each sample, and then average those. This way, you have
a permutation-invariant model which is still model-agnostic. Given some conditions on the width and depth of the network, 
these models can represent any permutation-invariant function.</p>

<p>There are quite some more papers that used other external memory mechanisms <d-cite key="munkhdalai2017meta"></d-cite>, or convolutions 
and attention <d-cite key="mishra2017simple"></d-cite>.</p>

<p>Unfortunately, these models are still quite limited in capabilities against “difficult” datasets, as you can see in the
table below.</p>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/bmml_results.png" class="img-fluid" alt="Alt text.">
    <figcaption class="figure-caption text-center">Results of a model trained with black-box meta-learning.</figcaption>
</figure>

<p>In summary, some benefits of black-box meta learning are its <strong>expressiveness</strong>, how easy it is to combine with a 
<strong>variety of learning problems</strong> (such as SL or RL). Nonetheless, it is a <strong>challenging optimization problem</strong> for a 
<strong>complex model</strong>, and it is often <strong>data-inefficient</strong>.</p>

<hr>

<h2 id="case-study-of-gpt-3">Case study of GPT-3</h2>

<p>With the rise of research on in-context learning, especially with foundation models, GPT-3 <d-cite key="brown2020language"></d-cite> is a good example of 
a black-box meta-learner, trained on language generation tasks. We can represent the task-specific datasets 
$\mathcal{D}_i^\mathrm{tr}$ as a sequence of characters, and $\mathcal{D}_i^\mathrm{test}$ as the following sequence 
of characters. This way, $\mathcal{D}_i^\mathrm{tr}$ is what the model is being conditioned on (its context), and 
$\mathcal{D}_i^\mathrm{test}$ is what it has to generate.</p>

<p>The meta-training dataset consists of crawled data from the internet, English-language Wikipedia, and two books corpora,
with a giant Transformer architecture as its network (175 billion parameters, 96 layers, 3.2M batch size).</p>

<p>For these datasets, there are a multitude of different tasks such as, but definitely not limited to, 
<strong>spelling correction</strong>, <strong>simple math problems</strong>, or <strong>translating between languages</strong>. By encoding every task as text,
the authors are able to obtain meta-training data incredibly easily.</p>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/gpt3_pipeline.png" class="img-fluid" alt="Alt text.">
    <figcaption class="figure-caption text-center">Abstract representation of the training meta-train pipeline of GPT-3.</figcaption>
</figure>

<p>In the case of GPT-3, text generation, also known as in-context learning, represents the inner loop of the optimization 
process. The outer loop represents the model optimizing across different tasks, which is very similar to the process that
we saw in the previous section.</p>

<p>With this model, you can easily do few-shot learning by adding examples in text form to the context of the model. Even 
through the model is far from perfect, its results are extremely impressive. It is also no oracle and can fail in 
unintuitive ways! If there is anything we have learned from recent research, it is that <strong>the choice of $\mathcal{D}^\mathrm{tr}$ 
at test time matters</strong> (welcome to the world of prompt engineering).</p>

<p>It is also interesting to think about what is needed for <strong>few-shot learning to emerge</strong> when training a model. This is 
an active research are, but it seems that (1) temporal correlation in your data with dynamic meaning of words, and (2) 
large model capacities definitely seems to make a difference here <d-cite key="chan2022data"></d-cite>.</p>

<hr>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Lars C.P.M. Quaedvlieg. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <d-bibliography src="/assets/bibliography/blog/cs330/2024-03-03-bbml-icl.bib"></d-bibliography>
    
    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-JKH10LEP3Y"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-JKH10LEP3Y');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
