<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-03-04T09:48:32+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lars C.P.M. Quaedvlieg</title><subtitle>The portfolio website of Lars C.P.M. Quaedvlieg
</subtitle><entry><title type="html">CS-330 Lecture 2: Transfer Learning and Meta-Learning</title><link href="http://localhost:4000/blog/2024/cs330-stanford-tl-ml/" rel="alternate" type="text/html" title="CS-330 Lecture 2: Transfer Learning and Meta-Learning" /><published>2024-03-03T00:00:00+01:00</published><updated>2024-03-03T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/cs330-stanford-tl-ml</id><content type="html" xml:base="http://localhost:4000/blog/2024/cs330-stanford-tl-ml/"><![CDATA[<p>The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two 
tasks to share a common structure, and start thinking about meta learning. If you missed the previous lecture, which was
about multi-task learning, you can head over <a href="/blog/2024/cs330-stanford-mtl/">here</a> to view it.</p>

<p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow 
of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p>

<p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_finetune_transfer_meta_learning_problem_setup_2023.pdf">here</a>.</p>

<h2 id="transfer-learning">Transfer learning</h2>

<p>In contrast to multi-task learning, which tackles several tasks $\mathcal{T}_1, \dots, \mathcal{T}_i$ simultaneously, 
transfer learning takes a sequential approach. It focuses on mastering a specific task $\mathcal{T}_b$ after the 
knowledge has been acquired from source task(s) $\mathcal{T}_a$. A common assumption is that $\mathcal{D}_a$ cannot be 
accessed during the transfer.</p>

<p>Transfer learning is a valid solution to multi-task learning, because it can sequentially apply knowledge from one task 
to another. This is unlike multi-task learning, which requires simultaneous learning of all tasks.</p>

<p>It is advantageous in the case of a large dataset $\mathcal{D}_a$, where continuous retraining is not feasible. Transfer
learning makes sense here by utilizing the acquired knowledge without the need for repetitive training on the large
dataset. Additionally, when you do not need to train two tasks simultaneously, you might opt to solve it them 
sequentially using transfer learning.</p>

<h3 id="transfer-learning-through-fine-tuning">Transfer learning through fine-tuning</h3>

\[\phi \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}_\mathrm{tr})\;.\]

<p>One method of transfer learning involves the fine-tuning of a pre-trained model with parameters $\theta$. This process 
starts with a model whose parameters have been <b>initially trained on a large, diverse dataset</b>, such as ImageNet <d-cite key="huh2016makes"></d-cite>.
The usefulness of fine-tuning lies in its ability to adapt these pre-trained parameters to a new task $\mathcal{T}_b$ by
continuing the training process with a dataset $\mathcal{D}_\mathrm{tr}$ specific to that task. Typically, this involves 
many iterations of gradient descent steps, where the pre-trained model's parameters $\theta$ are updated by moving in the
direction that minimizes the loss $\mathcal{L}$. This optimization process is depicted in the equation above.</p>

<p>When utilizing fine-tuning for transfer learning, there are several common design choices that are usually considered:</p>

<ul>
  <li>Opting for a <strong>lower learning rate</strong> to prevent the overwriting of the knowledge captured during pre-training.</li>
  <li>Employing even <strong>smaller learning rates for the earlier layers</strong> of the network, preserving more generic features.</li>
  <li>Initially <strong>freezing the early layers</strong> of the model, then gradually unfreezing them as training progresses.</li>
  <li><strong>Reinitializing the last layer</strong> to tailor it to the specifics of the new task.</li>
  <li><strong>Searching over hyperparameters</strong> using cross-validation to find the optimal configuration.</li>
  <li>Making smart choices about the <strong>model architecture</strong>.</li>
</ul>

<figure class="figure col-sm-11 float-right">
    <img src="/assets/img/blog/cs330/3/pretraining_datasets.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center"> Aggregate performance of a model across 10 finetuning datasets when it is (i) randomly 
    initialized (ii) pretrained on upstream corpus (BookWiki) (iii) pretrained on the finetuning dataset itself.</figcaption>
</figure>

<p>❗However, this common knowledge does not always hold true. For example, when using unsupervised pre-trained objectives,
you may not require diverse data for pre-training. The figure above shows that a model that was pretrained on the 
downstream dataset performs similarly to an upstream corpus such as BookWiki <d-cite key="krishna2022downstream"></d-cite>.</p>

<figure class="figure col-sm-11 float-right">
    <img src="/assets/img/blog/cs330/3/layer_tuning.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center"> How fine-tuning different layers has different effects.</figcaption>
</figure>

<p>Furthermore, depending on the downstream task, it may be better to tune the first or middle layers, rather than the last
layers. For example, for image corruption, it makes more sense to fine-tune the first layer of the model, since it’s
more of an input-level shift in data distribution <d-cite key="lee2022surgical"></d-cite>. The figure below shows more of these examples. Chelsea’s 
advice is to first <strong>train the last layer</strong>, and then <strong>fine-tune the entire network</strong> <d-cite key="kumar2022fine"></d-cite>, since fine-tuning can 
distort pre-trained features.</p>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/3/tl_dataset_size.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">The effect of fine-tuning with different dataset sizes.</figcaption>
</figure>

<p>However, one big disadvantage to fine-tuning is that <strong>it does not work well for very small target datasets</strong>. An 
example of this can be seen in the figure above. Luckily, this is where meta learning comes into play!</p>

<hr />

<h2 id="introduction-to-meta-learning">Introduction to meta learning</h2>

<p>With transfer learning, we initialize a model and then hope that it helps to solve the target task, by for example 
fine-tuning it. With meta-learning, we are asking the question of whether we can <strong>explicitly optimize for transferability</strong>.
Thus, given a set of training tasks, can we optimize the ability to learn these tasks quickly, so that we can learn <em>new</em> 
tasks quickly too.</p>

<p>When learning a task, we are very roughly mapping a task dataset to a set of model parameters through a function 
$\mathcal{D}^\mathrm{tr}_i \rightarrow \theta$. In meta learning, we are asking whether we can optimize this function
for a small $\mathcal{D}_i^\mathrm{tr}$.</p>

<p>There are two ways to view meta-learning algorithms:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <p><b>(1) Mechanistic view</b></p>
        <p>Construct a deep network that can read in an entire dataset and make predictions for new datapoints. Training this
        network uses a meta-dataset, which itself consists of many datasets, each for a different task.</p>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <p><b>(2) Probabilistic view</b></p>
        <p>Extract shared prior knowledge from a set of tasks that allows for efficient learning of new tasks. Then, learning a 
        new task uses this prior and a (small) training set to infer the most likely posterior parameters.</p>
    </div>
</div>

<h3 id="a-probabilistic-view-on-meta-learning">A probabilistic view on meta learning</h3>

<div>
<figure class="figure col-sm-5 float-right">
    <img src="/assets/img/blog/cs330/3/mtl_graphical_model.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Graphical model of multi-task- and meta-learning.</figcaption>
</figure>

<p>Expanding on the probabilistic (Bayesian) view of meta learning, let’s look at a graphical model for multi-task and
meta learning. To quickly recap what a graphical model represents, consider two random variables $X$ and $Y$. If there
is an arrow from $X$ to $Y$ in the graphical model, it means that $p(Y\vert X) \neq P(Y)$, meaning that the random 
variable $Y$ is dependent on $X$. Furthermore, you can nest certain variables (the rounded squared squares with $i$ 
and $j$) in the figure on the right, if you would like to repeat them for different indices.</p>
</div>

<p>Now we can start interpreting the graphical model for multi-task learning. Merging the training and testing sets, we 
immediately see that the target variable of datapoint $j$ for task $i$, denoted as $y_{i,j}$, is dependent on both the 
input data $x_{i,j}$ and the task-specific “true” parameter(s) $\phi_i$. The only difference between the training and 
testing data is that the target variables of the test dataset are not observed, whilst the others are all latent 
variables (unobserved). As we saw, for each task, we have task-specific true parameters $\phi_i$. However, if we share
some <strong>common structure</strong> between multiple tasks, we can condition these parameters on this common structure. Hence, 
$\theta$ defines the parameters related to the shared structure between different tasks.</p>

<p>This shared structure means that task parameters $\phi_{i_1}, \phi_{i_2}$ become independent when conditioning on the 
shared parameters $\theta$: $\phi_{i_1} \perp \phi_{i_2} \vert \theta$. Furthermore, the entropy of $p(\phi_i \vert \theta)$
is lower than $p(\phi_i)$, as there is less distributional noise from common structure.</p>

<p>Let’s now have a thought exercise. If we can identify $\theta$, then when should learning $\phi_i$ be faster than 
learning from scratch? Let’s think about one extreme. If the shared information fully describes the task-specific 
information, we would see that $p(\phi_i \vert \theta) = p(\phi_i \vert \phi_i) = 1$. From that, we can see that it is
faster if there is a lot of common information about the task that is captured by $\theta$. In a more general case, if 
the entropy $\mathcal{H}(p(\phi_i\vert\theta)) = 0$, meaning that you can predict $\phi_i$ with 100% accuracy given 
$\theta$, you can learn the fastest. However, this does not necessarily mean that $\phi_i = \theta$!</p>

<p>From all of this, we can define <strong>structure</strong> as a <strong>statistical dependence</strong> on <strong>shared latent information 
$\theta$.</strong> Let’s now see some examples of the type of information that $\theta$ may contain.</p>

<ul>
  <li>In a multi-task sinusoid problem, $\theta$ corresponds to the family of sinusoid functions, which is everything except the phase and amplitude.</li>
  <li>In a multi-language machine translation problem, $\theta$ corresponds to the family of all language pairs.</li>
</ul>

<p>Note that $\theta$ is <strong>narrower</strong> than the space of all possible functions!</p>

<p>We will discuss this probabilistic view on meta learning more in later lectures, but for the remainder of this lecture, 
we will switch back to the mechanistic view.</p>

<h3 id="how-does-meta-learning-work">How does meta learning work?</h3>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/3/ml_example.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Example of a meta-learning object classification problem.</figcaption>
</figure>

<p>Let’s consider an image classification problem, where you have different tasks. In the problem above, the different 
tasks contain different images to classify. For the (meta-)training process, we have tasks $\mathcal{T}_1, \mathcal{T}_2, \dots, \mathcal{T}_n$
and we would like to do meta-testing on a new task $\mathcal{T}_\mathrm{test}$. The goal is to learn to solve task 
$\mathcal{T}_\mathrm{test}$ more quickly than from scratch. We can then test after training on the few examples from 
the new (in this case testing) task $\mathcal{T}_\mathrm{test}$. Of course, this problem settings generalizes to any 
other machine learning problem like regression, language generation, skill learning, etc.</p>

<p>The <b>key assumption</b> here is that meta-training tasks and meta-testing tasks are drawn i.i.d. from the same task 
distribution $\mathcal{T}_1, \dots, \mathcal{T}_n,\mathcal{T}_\mathrm{test} \sim p(\mathcal{T})$, meaning that tasks 
must share structure.</p>

<p>Analogous to more data in machine learning, the more tasks, the better! You can say that meta learning is transfer 
learning with many source tasks.</p>

<p>The following is some terminology for different things you will hear when talking about meta learning:</p>

<ul>
  <li>The task-specific training set $\mathcal{D}_i^\mathrm{tr}$ is often referred to as the <em>support set</em> or the <em>context</em>.</li>
  <li>The task test dataset $\mathcal{D}_i^\mathrm{test}$ is called the <em>query set</em>.</li>
  <li><em>k-shot learning</em> refers to learning with <strong>k</strong> examples per class.</li>
</ul>

<h3 id="a-general-recipe-for-meta-learning-algorithms">A general recipe for meta learning algorithms</h3>

<p>Let’s formalize meta supervised learning in a mechanistic view. We are looking for a function 
$y^\mathrm{ts} = f_\theta(\mathcal{D}^\mathrm{tr}, x^\mathrm{ts})$, which is trained on the data $\{\mathcal{D}_i\}_{i=1,\dots,n}$. 
This formulation reduces the meta-learning problem to the design and optimization of $f_\theta$.</p>

<p>To approach a problem using meta learning, you will need to decide on two steps:</p>

<ol>
  <li>What is my form of $f_\theta(\mathcal{D}^\mathrm{tr}, x^\mathrm{ts})$?</li>
  <li>How do I optimize the meta-parameters $\theta$ with respect to the maximum-likelihood objective using meta-training data.</li>
</ol>

<p>The following lectures will focus on core methods for meta learning and unsupervised pre-trained methods!</p>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning" /><category term="course" /><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two tasks to share a common structure, and start thinking about meta learning.]]></summary></entry><entry><title type="html">CS-330 Lecture 3: Black-Box Meta-Learning &amp;amp; In-Context Learning</title><link href="http://localhost:4000/blog/2024/cs330-stanford-bbml-icl/" rel="alternate" type="text/html" title="CS-330 Lecture 3: Black-Box Meta-Learning &amp;amp; In-Context Learning" /><published>2024-03-03T00:00:00+01:00</published><updated>2024-03-03T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/cs330-stanford-bbml-icl</id><content type="html" xml:base="http://localhost:4000/blog/2024/cs330-stanford-bbml-icl/"><![CDATA[<p>The goal of this lecture is to learn how to <strong>implement black-box meta-learning</strong> techniques. We will also talk about a
<strong>case study of GPT-3</strong>! If you missed the previous lecture, which was about transfer learning by fine-tuning and meta
learning, you can head over <a href="/blog/2024/cs330-stanford-tl-ml/">here</a> to view it.</p>

<p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow 
of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p>

<p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_metalearning_bbox_2023.pdf">here</a>.</p>

<h2 id="black-box-adaptation-approaches">Black-box adaptation approaches</h2>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/omniglot.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Example of the Omniglot dataset.</figcaption>
</figure>

<p>The content of this section will build on the general recipe for meta-learning problems that we saw in the previous 
lecture. In order to explain it, we will use the example of the Omniglot dataset <d-cite key="lake2019omniglot"></d-cite>, which is a dataset of 1,623 
characters from 50 different alphabets. In this problem, every alphabet would refer to a different task. In our example,
we will do 3-way 1-shot learning, meaning that our sampled datasets consist of 3 classes with 1 example per class at 
every step. One iteration of the black-box meta-training process then has the following steps:</p>

<ol>
  <li>Sample task $\mathcal{T}_i$ or a mini-batch of tasks. In our case, this would correspond to generating the language(s).</li>
  <li>From the selected language(s), we sample disjoint datasets $\mathcal{D}_i^\mathrm{tr}$ and $\mathcal{D}_i^\mathrm{test}$ from $\mathcal{D}_i$. In our example, this will be a disjoint dataset with 3 samples of characters for every language alphabet.</li>
</ol>

<div>
<figure class="figure col-sm-5 6 float-right">
    <img src="/assets/img/blog/cs330/4/param_lstm.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Basic model architecture for black-box meta learning.</figcaption>
</figure>

<p>Now that we have these datasets, our goal is to train a neural network to represent $\phi_i = f_\theta(\mathcal{D}_i^\mathrm{tr})$. 
After computing these task parameters given a sampled training dataset, we can predict the test targets with $y^\mathrm{ts} = g_{\phi_i}(x^\mathrm{ts})$. 
An example of how such a model could work, is depicted in the figure above. Here, we are using a sequence model for 
$f_\theta$, which generates the parameters $\phi_i$. However, you can use all your fancy architectures that can handle 
a varying number of input sample. This is necessary due to varying dataset lengths.</p>
</div>

<p>After computing $y^\mathrm{ts}$, we can do backpropagation of the loss that is generated with the this test dataset. 
The full optimization objective is shown in the equation below:</p>

\[\min_\theta \sum_{\mathcal{T}_i} \sum_{(x,y) \sim \mathcal{D}^\mathrm{test}_i} - \log g_{\phi_i}(y\vert x) = \min_\theta \sum_{\mathcal{T}_i}\mathcal{L}(f_\theta(\mathcal{D}^\mathrm{tr}_i), \mathcal{D}^\mathrm{test}_i)\;.\]

<p>Notice that we are optimizing the parameters $\theta$. The task-specific parameters $\phi_i$ are generated by 
$f_\theta(\mathcal{D}_i^\mathrm{tr})$, and so they are not updated. Also note that the loss is calculated with 
respect to the sampled <strong>test dataset</strong>! This is no problem, since it makes sense to evaluate on new tasks for meta 
learning.</p>

<p>Now that you understand the architecture, we can write down the last two steps of the meta-training process:</p>

<ol>
  <li>Compute $\phi_i \leftarrow f_\theta(\mathcal{D}_i^\mathrm{tr})$.</li>
  <li>Update $\theta$ using $\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$.</li>
</ol>

<h3 id="a-more-scalable-architecture">A more scalable architecture</h3>

<p>However, we run into an issue. How do we let the model $f_\theta$ output another model’s parameters $\phi_i$? Not only 
can this be quite tricky to do, it also does not scale to larger parameter vectors $\phi_i$! Can you think of an 
alternative way of going this?</p>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/better_model.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">More scalable architecture for black-box meta-learning.</figcaption>
</figure>

<p>Instead of letting $f_\theta$ output $\phi_i$, we instead output a hidden state $h_i$, which is a low-dimensional 
vector that is supposed to represent contextual task information from the training dataset. If you recall the different
ways of conditioning that we saw for multi-task learning, you can see that we can train a model end-to-end by 
conditioning as $y^\mathrm{ts} = g_{\phi}(x^\mathrm{ts} \vert h_i)$. Now, notice that we have a general set of 
parameters $\phi$ for $g$; it does not need to be task-specific anymore, since we are already conditioning on task
information. In the figure above, $\theta$ are the parameters of the sequence model, and $\phi$ are the parameters of 
the convolutional network.</p>

<p>❗One problem that sometimes occurs with this architecture, is that the model learns to <strong>ignore conditioning on $h_i$.</strong>
In that case, it is essentially just learning to memorize, and not using the training dataset. In order to avoid that,
you can randomize the numerical label assignment to the target variables when sampling the datasets $\mathcal{D}^{tr}_i$
and $\mathcal{D}^{test}_i$. If the numerical label is different each time, it cannot just memorize the sample from the 
testing set.</p>

<h3 id="black-box-adaptation-architectures">Black-box adaptation architectures</h3>

<p>The architecture that we just presented was more-or-less first proposed on the Omniglot dataset at ICML in 2016 <d-cite key="santoro2016meta"></d-cite>.
It used LSTMs with Neural Turing Machines (which are not used anymore nowadays). Since then, a lot of new architectures 
have been proposed.</p>

<p>At ICML 2018, an architecture called the DeepSet architecture <d-cite key="garnelo2018conditional"></d-cite> was published. The idea is to pass all your dataset 
samples through a feedforward neural network to get an embedding of each sample, and then average those. This way, you have
a permutation-invariant model which is still model-agnostic. Given some conditions on the width and depth of the network, 
these models can represent any permutation-invariant function.</p>

<p>There are quite some more papers that used other external memory mechanisms <d-cite key="munkhdalai2017meta"></d-cite>, or convolutions 
and attention <d-cite key="mishra2017simple"></d-cite>.</p>

<p>Unfortunately, these models are still quite limited in capabilities against “difficult” datasets, as you can see in the
table below.</p>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/bmml_results.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Results of a model trained with black-box meta-learning.</figcaption>
</figure>

<p>In summary, some benefits of black-box meta learning are its <strong>expressiveness</strong>, how easy it is to combine with a 
<strong>variety of learning problems</strong> (such as SL or RL). Nonetheless, it is a <strong>challenging optimization problem</strong> for a 
<strong>complex model</strong>, and it is often <strong>data-inefficient</strong>.</p>

<hr />

<h2 id="case-study-of-gpt-3">Case study of GPT-3</h2>

<p>With the rise of research on in-context learning, especially with foundation models, GPT-3 <d-cite key="brown2020language"></d-cite> is a good example of 
a black-box meta-learner, trained on language generation tasks. We can represent the task-specific datasets 
$\mathcal{D}_i^\mathrm{tr}$ as a sequence of characters, and $\mathcal{D}_i^\mathrm{test}$ as the following sequence 
of characters. This way, $\mathcal{D}_i^\mathrm{tr}$ is what the model is being conditioned on (its context), and 
$\mathcal{D}_i^\mathrm{test}$ is what it has to generate.</p>

<p>The meta-training dataset consists of crawled data from the internet, English-language Wikipedia, and two books corpora,
with a giant Transformer architecture as its network (175 billion parameters, 96 layers, 3.2M batch size).</p>

<p>For these datasets, there are a multitude of different tasks such as, but definitely not limited to, 
<strong>spelling correction</strong>, <strong>simple math problems</strong>, or <strong>translating between languages</strong>. By encoding every task as text,
the authors are able to obtain meta-training data incredibly easily.</p>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/gpt3_pipeline.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Abstract representation of the training meta-train pipeline of GPT-3.</figcaption>
</figure>

<p>In the case of GPT-3, text generation, also known as in-context learning, represents the inner loop of the optimization 
process. The outer loop represents the model optimizing across different tasks, which is very similar to the process that
we saw in the previous section.</p>

<p>With this model, you can easily do few-shot learning by adding examples in text form to the context of the model. Even 
through the model is far from perfect, its results are extremely impressive. It is also no oracle and can fail in 
unintuitive ways! If there is anything we have learned from recent research, it is that <strong>the choice of $\mathcal{D}^\mathrm{tr}$ 
at test time matters</strong> (welcome to the world of prompt engineering).</p>

<p>It is also interesting to think about what is needed for <strong>few-shot learning to emerge</strong> when training a model. This is 
an active research are, but it seems that (1) temporal correlation in your data with dynamic meaning of words, and (2) 
large model capacities definitely seems to make a difference here <d-cite key="chan2022data"></d-cite>.</p>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning" /><category term="course" /><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to implement black-box meta-learning techniques. We will also talk about a case study of GPT-3!]]></summary></entry><entry><title type="html">CS-330 Lecture 1: Multi-Task Learning</title><link href="http://localhost:4000/blog/2024/cs330-stanford-mtl/" rel="alternate" type="text/html" title="CS-330 Lecture 1: Multi-Task Learning" /><published>2024-03-02T00:00:00+01:00</published><updated>2024-03-02T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/cs330-stanford-mtl</id><content type="html" xml:base="http://localhost:4000/blog/2024/cs330-stanford-mtl/"><![CDATA[<p>The goal of this lecture is to understand the key design decisions when building multi-task learning systems. Since I am
still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or 
whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p>

<p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_multitask_transfer_2023.pdf">here</a>.</p>

<h2 id="problem-statement">Problem statement</h2>

<p>We will first establish some notation that will be used throughout the course. Let’s first introduce the single-task 
supervised learning problem.</p>

\[\min_\theta \mathcal{L}(\theta, \mathcal{D}), \quad \text{s.t.} \quad \mathcal{D} = \{(x,y)_k\}\;.\]

<p>Here, $\mathcal{L}$ is the loss function, $\theta$ are the model parameters and $\mathcal{D}$ is the dataset. A typical 
example of a loss function would be the negative log-likelihood function $\mathcal{L}(\theta, \mathcal{D}) = - \mathbb{E}\left[\log f_\theta(y\vert x)\right]$.</p>

<p>We can formally define a <strong>task</strong> as follows<strong>:</strong></p>

\[\mathcal{T}_i := \{p_i(x), p_i(y\vert x), \mathcal{L}_i\}\;.\]

<p>Here, $p_i(x)$ is the input data distribution, $p_i(y\vert x)$ is the distribution of the target variable(s), and 
$\mathcal{L}_i$ is a task-specific loss function (can of course be the same for different tasks). The corresponding 
task datasets are $\mathcal{D}_i^\mathrm{tr} := \mathcal{D}_i$ and $\mathcal{D}_i^\mathrm{test}$.</p>

<p>Some examples of tasks:</p>

<ul>
   <li>Multi-task classification ($\mathcal{L_i}$ the same for each task)</li>
   <ul>
      <li>Per-language handwriting recognition.</li>
      <li>Personalized spam filter.</li>
   </ul>
   <li>Multi-label learning ($\mathcal{L_i}$ and $p_i(x)$ the same for each task)</li>
   <ul>
      <li>Face attribute recognition.</li>
      <li>Scene understanding.
      <div class="col-sm-5 mt-3 mt-md-0"><figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/weighted_mtl_objective.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>
</div>
      </li>
   </ul>
</ul>

<p>It is important to realize that $\mathcal{L}_i$ might change across tasks, for example when mixing discrete from
continuous data or if there are multiple metrics that you care about.</p>

<hr />

<h2 id="models-objectives-optimization">Models, objectives, optimization</h2>

<p>One way of helping a model identify different tasks would be to condition the model function by a task descriptor 
$z_i$: $f_\theta(y\vert x, z_i)$. This could be anything ranging from user features, language descriptions, or formal 
task specifications. The next subsections will focus on how to condition the model, which objective should be used, and 
how the objective should be optimized.</p>

<h3 id="model">Model</h3>

<figure class="figure col-sm-10 float-right">
    <img src="/assets/img/blog/cs330/2/mult_gating.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Network architecture for task-specific independent subnetworks.</figcaption>
</figure>

<p>Let’s first think about how we can condition on the task in order to share <strong>as little information</strong> as possible. The
answer to this is simple: you can create a function that uses multiplicative gating with a one-hot encoding of the task
. The model function would be $f_\theta(y \vert x, z_i) = \sum_j \mathbb{1}(z_i=j)f_{\theta_i}(x)$. This results in
independent training with a single network per tasks; there are no shared parameters. This can be seen in the figure above.</p>

<p>On the other extreme, you could simply concatenate $z_i$ with the input and/or activations in the model. In this case, 
all parameters are shared (except the ones directly following $z_i$, in case it is one-hot).</p>

<p>This give rise to a question: can you phrase the multi-task learning objective parameters $\theta = \theta_\mathrm{sh} 
\cup \theta_i$, where $\theta_\mathrm{sh}$ are shared parameters and $\theta_i$ are task-specific parameters? Our 
objective function becomes the following:</p>

\[\min_{\theta_\mathrm{sh}, \theta_1, \dots, \theta_T} \sum_{i=1}^T \mathcal{L}_i(\theta_\mathrm{sh} \cup \theta_i, \mathcal{D}_i)\;.\]

<p>In this case, choosing how to condition on $z_i$ is equivalent to choosing how and where to share model parameters. We 
will now look into some basic ways to condition a model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/concat_cond.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Concatenation-based conditioning.</figcaption>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/additive_cond.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Additive conditioning.</figcaption>
    </div>
</div>

<p><br />Can you see why additive conditioning in this way is equivalent to concatenation-based conditioning? Hint: think about 
how matrix multiplication splits the parameters when concatenating<d-footnote>You can find the solution to this question in the <a href="https://cs330.stanford.edu/materials/cs330_multitask_transfer_2023.pdf">lecture slides</a> (slide 13).</d-footnote>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/multi_head.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Multi-head architecture conditioning.</figcaption>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/mult_cond.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Multiplicative conditioning.</figcaption>
    </div>
</div>
<p><br /></p>

<p>One benefit of multiplicative conditioning is that you have this multiplicative gating, allowing more expressiveness 
per layer. It generalizes independent networks and independent heads.</p>

<p>There are more complex conditioning techniques, and a lot of research has gone into this topic, such as Cross-Stitch Networks <d-cite key="misra2016cross"></d-cite>, 
Multi-Task Attention Network <d-cite key="liu2019end"></d-cite>, Deep Relation Networks <d-cite key="dai2017detecting"></d-cite>, 
Perceiver IO <d-cite key="jaegle2021perceiver"></d-cite>, and more.</p>

<p>Unfortunately, these design choices are <strong>problem dependent</strong>, largely guided by <strong>intuition</strong> or <strong>knowledge</strong> about 
the problem, and currently more of an <strong>art</strong> than a science.</p>

<h3 id="objectives">Objectives</h3>

<p>We already saw a previous example of a multi-task objective function. Let’s start with the vanilla multi-task learning 
(MTL) objective: $\min_\theta \sum_{i=1}^T \mathcal{L}_i(\theta, \mathcal{D_i})$. Let’s now show some other ways to 
construct multi-task objective functions.</p>

<ol>
  <li>
    <p>Weighted multi-task learning (manually based on priority or dynamically adjust weights throughout training):</p>

\[\min_\theta \sum_{i=1}^T w_i \mathcal{L}_i(\theta, \mathcal{D_i})\;.\]
  </li>
  <li>
    <p>Minimax multi-task learning to optimize for the worst-case task loss (useful in robustness or fairness):</p>

\[\min_\theta \max_i \mathcal{L}_i(\theta, \mathcal{D_i})\;.\]
  </li>
  <li>
    <p>You can use various <strong>heuristics</strong> to construct your objective function. One example is to encourage gradients to have similar magnitudes across tasks.</p>
  </li>
</ol>

<h3 id="optimization">Optimization</h3>

<p>For the vanilla MTL objective, a basic training approach follows the following steps:</p>

<ol>
  <li>Sample mini-batch of tasks $\mathcal{B} = {\mathcal{T}_i}$.</li>
  <li>Sample mini-batch of datapoints for each task $\mathcal{D}^b_i \sim \mathcal{D}_i$.</li>
  <li>Compute mini-batch loss $\hat{\mathcal{L}}(\theta, \mathcal{B}) = \sum_{\mathcal{T}_k \in \mathcal{B}} \mathcal{L}_k(\theta, \mathcal{D}_k^b)$.</li>
  <li>Backpropagate the loss to compute $\nabla_\theta \hat{\mathcal{L}}$.</li>
  <li>Perform a step of gradient descent with some optimizer.</li>
  <li>Repeat from step 1.</li>
</ol>

<p>This process ensures that tasks are sampled uniformly, regardless of data quantities. However, it is important to ensure 
that the task labels, and the loss function, are on the same scale.</p>

<hr />

<h2 id="challenges">Challenges</h2>

<p>There are multiple challenges that come with multi-task learning.</p>

<ol>
  <li>
    <p><strong>Negative transfer</strong>: Sometimes independent subnetworks work better than parameter sharing. This could be due to <strong>optimization challenges</strong> (cross-task interference or tasks learning at different rates), or <strong>limited representational capacity</strong> (multi-task networks often need to be <em>much larger</em> than their single-task counterparts).</p>

    <p>In the case of negative transfer, you should share less across tasks. You can also add a regularization term to the objective function, to allow <em>soft parameter sharing</em>:</p>

\[\min_{\theta_\mathrm{sh}, \theta_1, \dots, \theta_T} \sum_{i=1}^T \mathcal{L}_i(\theta_\mathrm{sh} \cup \theta_i, \mathcal{D}_i) + \lambda \sum_{i^\prime = 1}^T \left\Vert \theta_i - \theta_i^\prime \right\Vert\;.\]

    <p>This allows for more fluid degrees of parameters sharing. However, it does add another set of hyperparameters, and it more memory intensive.</p>
  </li>
  <li><strong>Overfitting</strong>: You might not be sharing enough parameters. Since multi-task learning is equivalent to a form of regularization, the solution could be to share more parameters.</li>
  <li><strong>Having many tasks</strong>: You might wonder how to train all tasks together and which ones will be complementary. Unfortunately, no closed-form solution exists for measuring task similarity. Nevertheless, there are ways to approximate it from one training run <d-cite key="fifty2021efficiently"></d-cite> <d-cite key="xie2024doremi"></d-cite>.</li>
</ol>

<hr />

<h2 id="case-study-of-real-world-multi-task-learning">Case study of real-world multi-task learning</h2>

<p>In this case study, we will discuss the paper “Recommending What Video to Watch Next: A Multitask Ranking System” <d-cite key="zhao2019recommending"></d-cite>. They 
introduce a large scale multi-objective ranking system for recommending what video to watch next on an industrial video 
sharing platform. The system faces many real-world challenges, including the presence of multiple competing ranking 
objectives, as well as implicit selection biases in user feedback.</p>

<p>The framework is constructed as follows:</p>

<ul>
  <li><strong>Inputs</strong>: What the user is currently watching (query video) and user features</li>
</ul>

<p>The procedure is the following:</p>

<ol>
  <li>Generate a few hundred of <strong>candidate videos</strong> (by pooling videos from multiple candidate generation algorithms such as matching topics of the query video, videos frequently watched with the query video, and others).</li>
  <li><strong>Rank</strong> the candidates.</li>
  <li><strong>Serve</strong> the top ranking videos to the user.</li>
</ol>

<p>The central topic of this paper is the ranking system. The authors decide that the inputs to the ranking model are the 
<strong>query video</strong>, <strong>candidate video</strong>, and <strong>context features</strong>. The model attempts to output a weighted combination of 
<strong>engagement</strong> and <strong>satisfaction</strong> predictions, which results in a ranking score. The score weights are manually tuned.</p>

<div>
<figure class="figure col-sm-7 float-right">
    <img src="/assets/img/blog/cs330/2/expert_model.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Multi-gate Mixture-of-Expert architecture.</figcaption>
</figure>

<p>On choice for the model architecture is a “shared-bottom model”, which has some shared bottom layers which split into
separate heads for each task. However, this will harm learning when the correlation between tasks is low. Instead, they
opt for a form of soft-parameter sharing that they call <b>Multi-gate Mixture-of-Experts</b> (MMoE). As you can see in the
figure, this architecture allows different parts of the network to “specialize” in certain tasks as experts. For each
task, an attention-like score is computed that decides which combination of experts should be used.</p>
</div>

<p>Formally, let’s call the expert networks $f_i(x)$. We then decide which expert to use for input $x$ and task $k$ by 
computing $g^k(x) = \mathrm{softmax}(W_{g^k}x)$. The features are then computed from the selected experts as 
$f_k(x) = \sum_{i=1}^n g_{(i)}^k(x)f_i(x)$. The output can finally be denoted by $y_k = h^k(f^k(x))$.</p>

<p>In the paper, they trained them model in temporal order, running training continuously to consume newly arriving data. 
They perform online A/B testing in comparison to the production system based on some live metrics, and stress that
model <strong>computational efficiency matters</strong>.</p>

<div>
<figure class="figure col-sm-7 float-right">
    <img src="/assets/img/blog/cs330/2/paper_results.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Results from different model configurations.</figcaption>
</figure>

<p>From the results, you can see that this sort of architecture definitely helps. Furthermore, they found that there was a 
20% change of <b>gating polarization</b> during distributed training. This means that not all experts are utilized equally
and there is a bias to some expert(s). They utilized drop-out on the experts to counteract this problem.</p>
</div>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning" /><category term="course" /><summary type="html"><![CDATA[This is the first lecture of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the key design decisions when building multi-task learning systems.]]></summary></entry><entry><title type="html">CS-330: Deep Multi-Task and Meta Learning - Introduction</title><link href="http://localhost:4000/blog/2024/cs330-stanford-introduction/" rel="alternate" type="text/html" title="CS-330: Deep Multi-Task and Meta Learning - Introduction" /><published>2024-03-01T00:00:00+01:00</published><updated>2024-03-01T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/cs330-stanford-introduction</id><content type="html" xml:base="http://localhost:4000/blog/2024/cs330-stanford-introduction/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>The course <a href="https://cs330.stanford.edu/">CS 330: Deep Multi-Task and Meta Learning</a>, by <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, is taught
on a yearly basis and discusses the foundations and current state of multi-task learning and meta learning.</p>

<p><strong>:warning: Note:</strong> I am discussing the content of the edition in Fall 2023, which no longer includes reinforcement learning.
If you are interested in this, I will be auditing <a href="https://cs224r.stanford.edu/">CS 224R Deep Reinforcement Learning</a>
later this spring, which is also taught by <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>.</p>

<p>In an attempt to improve my writing skills and provide useful summaries/voice my opinions, I have decided to discuss 
the content of every lecture in this blog. In this post, I will give an overview of the course and why it is important 
for AI, especially now.</p>

<p>This course will focus on solving problems that are composed of multiple tasks, and studies how structure that arises from these multiple tasks can be leveraged to learn more efficiently/effectively, including:</p>

<ul>
  <li>Self-supervised pre-training for downstream few-shot learning and transfer learning.</li>
  <li>Meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly.</li>
  <li>Curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer.</li>
</ul>

<hr />

<h2 id="lectures">Lectures</h2>

<p>The lecture schedule of the course is as follows:</p>
<ol>
  <li><a href="/blog/2024/cs330-stanford-mtl/">Multi-task learning</a></li>
  <li><a href="/blog/2024/cs330-stanford-tl-ml/">Transfer learning &amp; meta learning</a></li>
  <li><a href="/blog/2024/cs330-stanford-bbml-icl/">Black-box meta-learning &amp; in-context learning</a></li>
  <li>Optimization-based meta-learning</li>
  <li>Few-shot learning via metric learning</li>
  <li>Unsupervised pre-training for few-shot learning (contrastive)</li>
  <li>Unsupervised pre-training for few-shot learning (generative)</li>
  <li>Advanced meta-learning topics (task construction)</li>
  <li>Variational inference</li>
  <li>Bayesian meta-learning</li>
  <li>Advanced meta-learning topics (large-scale meta-optimization)</li>
  <li>Lifelong learning</li>
  <li>Domain Adaptation and Domain Generalization</li>
  <li>Frontiers &amp; Open Challenges</li>
</ol>

<p>I am excited to start discussing these topics in greater detail! Check this page regularly for updates, since I will 
link to new posts whenever they are available!</p>

<hr />

<h2 id="why-multi-task-and-meta-learning">Why multi-task and meta-learning?</h2>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/1/robotics_example.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Robots are embodied in the real world, and must generalize across tasks. In order to do so, they need some common sense 
understanding and supervision can’t be taken for granted.</p>

<p>Earlier robotics and reinforcement research mainly focused on problems that required learning a task from scratch. This 
problem is even present in other fields, such as object detection or speech recognition. However, as opposed to these 
problems, <strong>humans are generalists</strong> that exploit common structures to solve new problems more efficiently.</p>

<p>Going beyond the case of generalist agents, deep multi-task and meta learning useful for any problems where a <strong>common 
structure</strong> can benefit the efficiency or effectiveness of a model. It can be impractical to develop models for each
specific task (e.g. each robot, person, or disease), especially if the data that you have access to for these individual
tasks is <strong>scarce</strong>.</p>

<p>If you need to <strong>quickly learn something new</strong>, you need to utilize prior experiences (e.g. few-shot learning) to make 
decisions.</p>

<p>But why now? Right now, with the speed of research advancements in AI, many researchers are looking into utilizing 
multi-model information to develop their models. Especially in robotics, foundation models seem <strong>the</strong> topic in 2024,
and many advancements have been made in the past year <d-cite key="zhao2023learning"></d-cite>, <d-cite key="open_x_embodiment_rt_x_2023"></d-cite>, <d-cite key="octo_2023"></d-cite>, <d-cite key="brohan2023rt"></d-cite>.</p>

<hr />

<h2 id="what-are-tasks">What are tasks?</h2>

<p>Given a dataset $\mathcal{D}$ and loss function $\mathcal{L}$, we hope to develop a model $f_\theta$. Different tasks 
can be used to train this model, with some simple examples being objects, people, objectives, lighting conditions, 
words, languages, etc.</p>

<p>The <strong>critical assumption</strong> here is that different tasks must share some common structure. However, in practice, this 
is very often the case, even for tasks that seem unrelated. For example the laws of physics and the rules of English
can be shared among many tasks.</p>

<ol>
  <li>The multi-task problem: Learn <strong>a set of tasks</strong> more quickly or more proficiently than learning them independently.</li>
  <li>Given data on previous task(s), learn <strong>a new task</strong> more quickly and/or more proficiently.</li>
</ol>

<blockquote>
  <p>Doesn’t multi-task learning reduce to single-task learning?</p>
</blockquote>

<p>This is indeed the case when aggregating data across multiple tasks, which is actually one approach to multi-task 
learning. However, what if you want to learn new tasks? And how do you tell the model which task to do? And what if 
aggregating doesn’t work?</p>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning" /><category term="course" /><summary type="html"><![CDATA[I have been incredibly interested in the recent wave of multimodal foundation models, especially in robotics and sequential decision-making. Since I never had a formal introduction to this topic, I decided to audit the Deep Multi-Task and Meta Learning course, which is taught yearly by Chelsea Finn at Stanford. I will mainly document my takes on the lectures, hopefully making it a nice read for people who would like to learn more about this topic!]]></summary></entry></feed>