<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-02-26T11:19:00+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lars C.P.M. Quaedvlieg</title><subtitle>The portfolio website of Lars C.P.M. Quaedvlieg
</subtitle><entry><title type="html">Foundations of Deep Learning Series - Introduction</title><link href="http://localhost:4000/blog/2024/introduction/" rel="alternate" type="text/html" title="Foundations of Deep Learning Series - Introduction" /><published>2024-02-26T00:00:00+01:00</published><updated>2024-02-26T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/introduction</id><content type="html" xml:base="http://localhost:4000/blog/2024/introduction/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>The <a href="https://theory.epfl.ch/readinggroup/">Foundations of Deep Learning</a> reading group at EPFL is organized by 
<a href="https://theory.epfl.ch/kapralov/">Michael Kapralov</a> and <a href="https://theory.epfl.ch/osven/">Ola Svensson</a> from February 
28th to May 29th.</p>

<p>The goal of the group is to delve into recent works that aim to lay down a theoretical foundation for deep 
neural networks, tackling the challenges presented by their significant computational demands, the opacity of their 
outputs, and the privacy concerns they raise. This initiative is driven by the recent advances in machine learning that
have led to notable breakthroughs in fields such as natural language processing and vision.</p>

<p>In this series, I will discuss every session of this meeting group, talking about current advancements in state-of-the-art
deep learning research. I hope to do things like review papers, analyze groundbreaking techniques, and share insights 
on overcoming the challenges that come with deep learning nowadays.</p>

<hr />

<h2 id="topics">Topics</h2>

<p>The reading group will sequentially discuss 4 research topics:</p>
<ol>
  <li><strong>Transformers</strong>: the transformer architecture, as well as (some of) the recent works on subquadratic attention mechanisms (xformers, State Space models, Hyena etc).
    <ul>
      <li>Basics of the Transformer architecture.</li>
      <li>Xformers (subquadratic attention mechanisms).</li>
      <li>More xformers (polysketchformer, hyperattention).</li>
      <li>State space models.</li>
    </ul>
  </li>
  <li><strong>Which functions can transformers learn</strong>: classes of functions that transformers can provably learn in-context.
    <ul>
      <li>Which functions can transformers learn in-context (RASP, RASP-L)</li>
      <li>In-context learning of linear regression, part i.</li>
      <li>In-context learning of linear regression, part ii.</li>
    </ul>
  </li>
  <li><strong>Graph neural networks</strong>: expressivity of graph neural networks (connections to the Weissfeiler-Lehman test), information bottlenecks in graph neural networks.
    <ul>
      <li>Graph embeddings.</li>
      <li>Basics of graph neural networks.</li>
      <li>Graph neural networks and the Weisfeiler Lehmann test.</li>
    </ul>
  </li>
  <li><strong>Reinforement learning</strong>: algorithmic foundations of reinforcement learning, including Markov Decision Processes and Monte Carlo Tree Search.
    <ul>
      <li>Reinforcement learning, Markov Decision Processes, Monte-Carlo tree search.</li>
    </ul>
  </li>
  <li><strong>Other</strong>.
    <ul>
      <li>ADAM.</li>
      <li>Differentially private training of neural networks.</li>
    </ul>
  </li>
</ol>

<p>I am excited to start discussing these topics in greater detail! Check this page regularly for updates!</p>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="foundations-of-deep-learning" /><category term="paper-review" /><summary type="html"><![CDATA[In this series, in which I hope to kick off this blog, I will create detailed posts on the Foundations of Deep Learning reading group at EPFL. This introduction contains information about the content and technicalities.]]></summary></entry></feed>