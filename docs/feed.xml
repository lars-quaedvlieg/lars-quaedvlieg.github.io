<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-03-19T21:33:27+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lars C.P.M. Quaedvlieg</title><subtitle>The portfolio website of Lars C.P.M. Quaedvlieg
</subtitle><entry><title type="html">CS-330 Lecture 6: Unsupervised Pre-Training: Contrastive Learning</title><link href="http://localhost:4000/blog/2024/cs330-stanford-upt-fsl-cl/" rel="alternate" type="text/html" title="CS-330 Lecture 6: Unsupervised Pre-Training: Contrastive Learning" /><published>2024-03-16T00:00:00+01:00</published><updated>2024-03-16T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/cs330-stanford-upt-fsl-cl</id><content type="html" xml:base="http://localhost:4000/blog/2024/cs330-stanford-upt-fsl-cl/"><![CDATA[<p>The goal of this lecture is to understand the intuition, design choices, and implementation of <strong>contrastive learning</strong> 
for unsupervised representation learning. We will also talk about the relationship between contrastive learning and meta 
learning! If you missed the previous lecture, which was about non-parametric few-shot learning, you can head over <a href="/blog/2024/cs330-stanford-fsl-ml/">here</a> 
to view it.</p>

<p>As always, since I am still quite new to this blogging thing, reach out to me if you have any feedback on my writing, 
the flow of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p>

<p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_contrastive_2023.pdf">here</a>.</p>

<h2 id="quick-introduction">Quick introduction</h2>

<figure class="figure col-sm-12">
 <img src="/assets/img/blog/cs330/7/fine-tune-example.png" class="img-fluid" alt="Alt text." />
 <figcaption class="figure-caption text-center">Example of the amount of data needed for transfer learning through fine-tuning.</figcaption>
</figure>

<p>So far we have talked about the idea of few-shot learning via meta learning. In this problem, you are given a set of 
tasks $\mathcal{T}_1, \cdots, \mathcal{T}_n$ to train on, and wish to solve a new task $\mathcal{T}_\mathrm{test}$ more 
quickly, effectively, and stably. Before starting with meta learning, we discussed the idea of using transfer learning 
via fine-tuning for this problem, but the performance of this method is very dependent on the amount of data, as you can
see on in figure above. Instead, we proposed three different types of meta learning to help quickly adapt to new tasks: 
black-box meta learning, optimization-based meta learning, and non-parametric meta learning.</p>

<p>These methods were shown to work especially well when there are <strong>many tasks available</strong> for a problem. But, when you only have few tasks, meta learning might not be a good approach to the problem due to risks of overfitting and having insufficient diversity in your data. Let’s take this even further. What if you only have <strong>one batch</strong> of <strong>unlabelled data</strong>?</p>

<figure class="figure col-sm-12">
 <img src="/assets/img/blog/cs330/7/unsupervised-pretraining.png" class="img-fluid" alt="Alt text." />
 <figcaption class="figure-caption text-center">The process of doing unsupervised pre-training for few-shot learning.</figcaption>
</figure>

<p>In this case, meta learning might not be a good approach to the problem. Instead, we will look into <strong>unsupervised representation learning for few-shot learning</strong>. In the figure above, we describe the process of training a model for this problem on a high level. Given a dataset of unlabelled data ${x_i}$, we want to do unsupervised pre-training to get an initial model. Once we have obtained this model, we then wish to fine-tune it on a task-specific dataset $\mathcal{D}_j^\mathrm{tr}$, to get a task-specific predictor.</p>

<p>You might have already noticed that this procedure is very similar to the way that <strong>large language models</strong> are trained. They are first pre-trained on a huge corpus of language data, and then fine-tuned for specific purposes (i.e. alignment, mathematics, etc.).</p>

<p>In this course, we will talk about two approaches to this problem:</p>

<ol>
  <li>Contrastive learning.</li>
  <li>Reconstruction-based methods.</li>
</ol>

<p>In this post, we will focus on contrastive learning, and we will discuss the reconstruction-based methods in the next one!</p>

<h2 id="contrastive-learning">Contrastive learning</h2>

<div>
<figure class="figure col-sm-3 float-right">
    <img src="/assets/img/blog/cs330/7/similar-reps.png" class="img-fluid" alt="Alt text." />
</figure>

<p>The idea behind contrastive learning is that <b>similar examples should have similar representations</b>, and different
examples should have different representations. When you have a batch of unsupervised data, you can decide on a semantic
meaning of similarity and then learn the data representations as embeddings from a model. The steps would roughly be as 
follows:</p>

<ol>
   <li>Select or generate examples that are semantically similar.</li>
   <li>Train an encoder where similar examples are closer in the representation space than non-similar examples.</li>
</ol>
</div>

<p>Let’s start out with a simple approach. We are trying to learn an model $f_\theta(x)$, which embeds a datapoint $x$ into some representation. As a loss function, we decide use the following:</p>

\[\min_\theta \sum_{(x_i, x_j)}\Vert f_\theta(x_i) - f_\theta(x_j)\Vert^2\;.\]

<div>
<figure class="figure col-sm-5 float-right">
    <img src="/assets/img/blog/cs330/7/basic-embed-space.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Abstract example of the embedding space.</figcaption>
</figure>

<p>This loss function tries to minimize the distance of the embeddings of <b>similar datapoints</b> $x_i$ <b>and</b> $x_j$. 
However, do you think this loss function performs well? Well, you might be able to see that one possible optimal solution
to this loss function would just be to let $f_\theta(x) = 0$. This would mean that <b>all</b> datapoints are mapped to the 
same representation, even very different datapoints. For this reason, the loss should also incorporate an element to
<b>push apart differing samples</b>. You need to both compare and contrast!</p>

</div>

<p>We present this idea in the figure on the right. In the embedding space, similar samples should be close, whilst differing samples should should be far apart. The key design choices here are choosing what to compare/contrast, and which contrastive loss you use.</p>

<p>Whilst the ideas work for all kinds of unlabelled data, we will focus on images (or videos) in the remainder of this post. Recalling that similar examples should have similar representations, we discuss a few ways to measure similarity in images.</p>

<p>The most straightforward way to assign similarity is by looking at class labels. This is very related to the Siamese 
networks and Prototypical networks that we saw in the previous post. However, for unsupervised data, this is not possible.
Instead, there are many different approaches that <strong>create new samples from one sample</strong>. Below are some examples <d-cite key="oord2018representation"></d-cite><d-cite key="chen2020simple"></d-cite>.</p>

<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
      <p><b>Patch-based.</b></p>
      <figure class="figure col-sm-12">
         <img src="/assets/img/blog/cs330/7/patches.png" class="img-fluid" alt="Alt text." />
      </figure>

      <p>Given an image, it is possible to split it into image patches, and to let image patches that are close to each other have a similar representation.</p>
   </div>
   <div class="col-sm mt-3 mt-md-0">
      <p><b>Augmentation-based.</b></p>
      <figure class="figure col-sm-12">
         <img src="/assets/img/blog/cs330/7/augments.png" class="img-fluid" alt="Alt text." />
      </figure>

      <p>Given an image, it is also possible to augment it in some way (i.e. by flipping, cropping, etc.), and letting those sample be similar to each other.</p>
   </div>
   <div class="col-sm mt-3 mt-md-0">
      <p><b>Temporally-based.</b></p>
      <figure class="figure col-sm-12">
         <img src="/assets/img/blog/cs330/7/videos.png" class="img-fluid" alt="Alt text." />
      </figure>

      <p>Given a video, it is <i>often</i> possible to let frames that are temporally close have a similar representation. Of course this depends on the nature of the video.</p>
   </div>
</div>

<p>As you can see, defining similarity is usually pretty problem-specific. A simple example in text would be something like <em>bag of words</em> depending on the task, or permutations with a similar semantic meaning.</p>

<div>
<figure class="figure col-sm-4 float-right">
    <img src="/assets/img/blog/cs330/7/mid-embed-space.png" class="img-fluid" alt="Alt text." />
</figure>

<p>Now that we have a way of defining similarity across samples, we can take a look at modifying the loss function to push
apart differing samples. One common loss function is the <b>triplet loss</b>, introduced in <d-cite key="schroff2015facenet"></d-cite>, which simply tries to
push away unrelated samples:</p>

</div>

\[\min_\theta \sum_{(x, x^+, x^-)}\max(0,\Vert f_\theta(x) - f_\theta(x^+)\Vert^2 - \Vert f_\theta(x) - f_\theta(x^-)\Vert^2 + \epsilon)\;.\]

<p>If you only consider $l_\theta(x, x^+, x^-) = \Vert f_\theta(x) - f_\theta(x^+)\Vert^2 - \Vert f_\theta(x) - f_\theta(x^-)\Vert^2$, this loss function would be unbounded, since it can decrease indefinitely. By introducing $\max(0, \cdots + \epsilon)$, you ensure that the values of $l_\theta(x, x^+, x^-)$ that affect the loss are bounded up to some margin $-\epsilon$. This implicitly defines <strong>how far apart</strong> you want your samples to be when comparing related versus unrelated samples.</p>

<p>This approach is <em>very</em> similar to Siamese networks, which classifies a pair $(x, x^\prime)$ as the same class if $\Vert f(x) - f(x^\prime)\Vert^2$ is small. The key difference is that contrastive learning <strong>learns a metric space</strong>, and not just a classifier.</p>

<p>Unfortunately, the Triplet loss has a downside: In order for it to be effective, you need to find <em>difficult</em> negatively similar examples, which can be very challenging. It is important to find difficult negative samples, since very obviously different ones will already be far apart and have a zero loss, meaning the model is not going to be learning anything from that negative sample.</p>

<div>
<figure class="figure col-sm-5 float-right">
    <img src="/assets/img/blog/cs330/7/adv-embed-space.png" class="img-fluid" alt="Alt text." />
</figure>

<p>One approach to finding difficult negative samples is called <b>hard negative mining</b>. It essentially just looks 
through a list of negative samples and tries to see which ones are close your sample in the embedding space. This brings
us to the idea of <b>sampling multiple negatives</b> in order to contrast with more difficult negative samples. This is 
depicted in the figure on the right.</p>

</div>

<p>The loss function then becomes an $N$-way classification problem, and it generalizes the triplet loss to using multiple negatives:</p>

\[\mathcal{L}_\mathrm{N-way}(\theta) = -\sum_z \log \left[ \frac{\exp(-d(z, z^+))}{\sum_i\exp(-d(z, z_i^-)) + \exp(-d(z, z^+))} \right]\;.\]

<p>Notice that the goal of this loss is to distinguish the similar sample from all of the negatives with some distance measure of your learned metric space $d(\cdot, \cdot)$, such as a Euclidean loss or negative cosine similarity.</p>

<p>This approach was taken in <d-cite key="sohn2016improved"></d-cite> and <d-cite key="chen2020simple"></d-cite>, but in practice people often use a slight modification of this loss function, which is shown below:</p>

\[\displaylines{\mathcal{L}(\theta) = -\sum_z \log \left[ \frac{\exp(-d(z, z^+))}{\sum_i\exp(-d(z, z_i^-))} \right] \cr
= \sum_z \left[\exp(-d(z, z^+)) + \log\sum_i\exp(-d(z, z_i^-))\right]\;.}\]

<p>This loss is usually preferred, since you really only want to push away negative examples, not the similar one as well. As you can see in the equivalent formula shown above, this is exactly what it is doing.</p>

<h3 id="the-simclr-algorithm">The SimCLR algorithm</h3>

<div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/7/simclr.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Training process of the SimCLR algorithm.</figcaption>
</figure>

<p>We will now talk about a way of sampling negative examples so we can compute this loss. There is an algorithm called 
<b>SimCLR</b> which is proposed in <d-cite key="chen2020simple"></d-cite>, that does exactly this. We visualize this approach in the figure on the right.
It is composed of the following steps:</p>

<ol>
<li>Sample a minibatch of examples $x_1, \cdots, x_N$.</li>
<li><b>Augment</b> each example <i>twice (design choice)</i> to get $\tilde{x}_1, \cdots, \tilde{x}_N, \tilde{x}_{N+1}, \cdots, \tilde{x}_{2N}$.</li>
<li><b>Embed</b> examples with $f_\theta$ to get $\tilde{z}_1, \cdots, \tilde{z}_N, \tilde{z}_{N+1}, \cdots, \tilde{z}_{2N}$.</li>
<li>Compute all <b>pairwise distances</b> $d(z_i, z_j) = -\frac{z_i^Tz_j}{\Vert z_i\Vert\Vert z_j\Vert}$ (negative cosine similarity).</li>
<li>Update <b>pairwise distances</b> $\theta$ with respect to $\mathcal{L}_\mathrm{N-way}(\theta)$.</li>
</ol>

</div>

<div>
<figure class="figure col-sm-5 float-left">
    <img src="/assets/img/blog/cs330/7/simclr-perf.png" class="img-fluid" alt="Alt text." />
</figure>

<figure class="figure col-sm-5 float-left">
    <img src="/assets/img/blog/cs330/7/simclr-efficiency.png" class="img-fluid" alt="Alt text." />
</figure>
</div>

<p><b>After pre-training</b> the function $f_\theta$, we can either train a classifier on top of the representations that it 
produces, or choose to fine-tune the entire network. The performance of this method was benchmarked on ImageNet 
classification, where the model was fine-tuned using only $1$% of all labels (~$12.8$ images per class) or $10$% of all
labels. The other part of the dataset was used as unsupervised pre-training data. It shows a substantial improvement 
over training from scratch, and also improvements over other methods, especially in the $1$% label setting.</p>

<p>In their experiments, they did note that it was important to use a <b>large batch size</b> (larger than 256), since it 
leads to longer needed training (more than 600 epochs) in order to get a good performance.</p>

<h3 id="theoretical-properties-of-contrastive-learning">Theoretical properties of contrastive learning</h3>

<p>One reason that <strong>contrastive learning needs a large batch size</strong>, is that the summation over the entire dataset in the $\mathcal{L}_\mathrm{N-way}(\theta)$ loss function will dominate for very close samples. However, if your batch size is too small, you might not include those similar hard examples. This is related to the previous problem of subsampling hard negatives. We will show this mathematically below.</p>

<p>We will rewrite the loss function using a minibatch $\mathcal{B}$ and find a lower bound using <strong>Jensen’s inequality</strong>:</p>

\[\displaylines{\exp(-d(z, z^+)) + \log\sum_n\exp(-d(z, z_n^-)) \cr
\geq\exp(-d(z, z^+)) + \sum_{\mathcal{B}} \log\sum_{n \in \mathcal{B}}\exp(-d(z, z_n^-))\;.}\]

<p>This shows that our training objective that uses minibatches actually solves a <strong>lower bound on the original objective</strong>.
This means that we might not actually be minimizing our original objective. However, the larger the batch size, the 
closer the lower bound gets to the original objective function. Can you see why? <d-footnote>Answer: We can express
$\sum_n = \sum_\mathcal{B}\sum_{n\in\mathcal{B}}$. If you let you minibatch be the entire dataset, then we only have
one minibatch, so the $\sum_{\mathcal{B}}$ in the lower bound will disappear and turn into objective function. However,
in the worst case with a batch size of 1, we have only one value for $\sum_{n \in \mathcal{B}}$ and many sums over
minibatches $\sum_{\mathcal{B}}$, meaning that the lower bound will be the further away from the initial objective. </d-footnote></p>

<h3 id="recent-works-in-contrastive-learning">Recent works in contrastive learning</h3>

<p>There are some papers that try to tackle the problem of optimizing this lower bound:</p>

<ul>
  <li>One idea is to store representations from previous batches using a form of momentum during training. It’s not completely correct, but they show it obtains good results with a batch size of $256$ <d-cite key="he2020momentum"></d-cite>.</li>
  <li>It is also possible to predict representations of the same image under different augmentation. In this case, you do not require any negatively similar examples <d-cite key="grill2020bootstrap"></d-cite>. It’s more of a predictive approach, but does not have a nice contrastive representation.</li>
</ul>

<figure class="figure col-sm-12">
    <img src="/assets/img/blog/cs330/7/imagenet.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Progress over the years on the ImageNet benchmark for self-supervised learning.</figcaption>
</figure>

<p>The image above shows results on the ImageNet benchmark over the past years, and contrastive methods (i.e. MoCo v3) are still close to <strong>state-of-the-art for self-supervised pre-training for visual data</strong>.</p>

<p>In this post we have mainly focussed on augmentation-based methods. However, for many applications, we do not have well-engineered augmentations.</p>

<ol>
  <li>A recent work <d-cite key="tamkin2020viewmaker"></d-cite> at ICLR in 2021 tries to <em>learn</em> the augmentations in an adversarial manner. It is competitive with SimCLR on image data and obtains good results on speech and sensor data.</li>
  <li>
    <p>Furthermore, <em>time-contrastive learning</em> on videos has been shown as effective for robotics pre-training, as presented in a paper <d-cite key="nair2022r3m"></d-cite> presented at CoRL in 2022. The method of this paper has been depicted in the figure below.</p>

    <figure class="figure col-sm-12">
 <img src="/assets/img/blog/cs330/7/time-constrastive-learning.png" class="img-fluid" alt="Alt text." />
 <figcaption class="figure-caption text-center">Process of the time-contrastive learning for robotics pre-training paper.</figcaption>
</figure>
  </li>
  <li>
    <p>Finally, the popular CLIP paper <d-cite key="agarwal2021evaluating"></d-cite> uses <em>image-text</em> contrastive pre-training to produce robust zero-shot models. It learns a representation of images and a representation of text, and can tell which images and captions go together (positive samples), and which ones should be pushed apart (negative samples). It shows good zero-shot transfer to out-of-distribution tasks.</p>

    <figure class="figure col-sm-12">
    <img src="/assets/img/blog/cs330/7/clip.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Example of how CLIP works and the performance on out-of-distribution tasks.</figcaption>
</figure>
  </li>
</ol>

<p>In summary, contrastive learning is a general and effective framework to do unsupervised pre-trained for few-shot adaptation. It does not require generative modelling and can incorporate domain knowledge through augmentations and similarity. However, it can be difficult to select negative samples, it often requires a large batch size for training, and is currently most successful with augmentations.</p>

<h3 id="contrastive-learning-as-meta-learning">Contrastive learning as meta learning</h3>

<p>Many of the equations that we saw in this post look similar to the ones that we saw in the previous post about non-parametric meta learning. It is actually possible to create a meta learning algorithm that works <em>similarly</em> to the contrastive approaches that we have seen today. Let’s formulate the problem as a meta learning problem:</p>

<ol>
  <li>Given an unlabelled dataset ${x_i}$.</li>
  <li>Create a class $y_i$ from each datapoint via data augmentation $\mathcal{D}_i := \{\tilde{x}_i, \tilde{x}_i^\prime, \cdots\}.$</li>
  <li>Run any meta learning algorithm on this dataset.</li>
</ol>

<p>There is a paper that goes in depth into similarities of SimCLR with Prototypical networks for meta learning, and shows the methods differ in the following ways:</p>

<ul>
  <li>SimCLR samples <em>one task</em> per minibatch, whereas meta learning usually samples multiple.</li>
  <li>SimCLR compares <em>all pairs</em> of samples, whereas meta learning compares query examples only to support examples and not to query other examples.</li>
</ul>

<p>In the table below, they also show that both representations transfer similarly well between different datasets.</p>

<figure class="figure col-sm-12">
 <img src="/assets/img/blog/cs330/7/meta-learning-ref.png" class="img-fluid" alt="Alt text." />
 <figcaption class="figure-caption text-center">Performance of Prototypical networks and SimCLR on different unsupervised few-shot learning problems.</figcaption>
</figure>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning" /><category term="course" /><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the intuition, design choices, and implementation of contrastive learning for unsupervised representation learning. We will also talk about the relationship between contrastive learning and meta learning!]]></summary></entry><entry><title type="html">CS-330 Lecture 5: Few-Shot Learning via Metric Learning</title><link href="http://localhost:4000/blog/2024/cs330-stanford-fsl-ml/" rel="alternate" type="text/html" title="CS-330 Lecture 5: Few-Shot Learning via Metric Learning" /><published>2024-03-14T00:00:00+01:00</published><updated>2024-03-14T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/cs330-stanford-fsl-ml</id><content type="html" xml:base="http://localhost:4000/blog/2024/cs330-stanford-fsl-ml/"><![CDATA[<p>The goal of this lecture is to understand the third form of meta learning: <strong>non-parametric few-shot learning</strong>. We will
also compare the three different methods of meta learning. Finally, we give practical examples of meta learning, in
domains such as <strong>imitation learning</strong>, <strong>drug discovery</strong>, <strong>motion prediction</strong>, and <strong>language generation</strong>! If you
missed the previous lecture, which was about optimization-based meta learning, you can head over 
<a href="/blog/2024/cs330-stanford-obml/">here</a> to view it.</p>

<p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow
of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p>

<p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_nonparametric_2023.pdf">here</a>.</p>

<h2 id="quick-recap">Quick recap</h2>

<p>So far, we have discussed two approaches to meta learning: black-box meta learning, and optimization-based meta learning.</p>

<ol>
  <li>
    <p><a href="/blog/2024/cs330-stanford-bbml-icl/">Black-box meta learning</a>.</p>

    <figure class="figure col-sm-12">
 <img src="/assets/img/blog/cs330/6/bbml_recap.png" class="img-fluid" alt="Alt text." />
 <figcaption class="figure-caption text-center">Computation pipeline for black-box meta learning.</figcaption>
</figure>

    <p>In <em>black-box meta learning</em>, we attempt to train some sort of meta-model to output task-specific parameters or contextual information, which can then be used by another model to solve that task. We saw that this method is <strong>very expressive</strong> (e.g. it can model many tasks). However, it also requires solving a <strong>challenging optimization problem</strong>, which is incredibly data-inefficient.</p>
  </li>
  <li>
    <p><a href="/blog/2024/cs330-stanford-obml/">Optimization-based meta learning</a>.</p>

    <figure class="figure col-sm-12">
 <img src="/assets/img/blog/cs330/6/obml_recap.png" class="img-fluid" alt="Alt text." />
 <figcaption class="figure-caption text-center">Computation pipeline for optimization-based meta learning.</figcaption>
</figure>

    <p>We then talked about <em>optimization-based meta learning</em>, which embeds an optimization process within the inner learning process. This way, you can learn to find parameters to a model such that optimizing these parameters to specific tasks is as effective and efficient as possible. We saw that model-agnostic meta learning <strong>preserves expressiveness</strong> over tasks, but it remains <strong>memory-intensive</strong>, and requires solving a <strong>second-order optimization</strong> problem.</p>
  </li>
</ol>

<h2 id="non-parametric-few-shot-learning">Non-parametric few-shot learning</h2>

<p>In the previous two approaches to meta learning, we only talked about parametric methods <d-footnote>A parametric model assumes a specific form for the underlying function between variables, using a finite number of parameters, while a non-parametric model makes fewer assumptions about the function form, potentially using an infinite number of parameters to model the data more flexibly.</d-footnote>. However, what if we can avoid the optimization process in the inner learning loop of optimization-based meta learning methods? If this is possible, we do not have to solve a second-order optimization problem anymore. For this reason, we will look into replacing the parametric models in the inner learning loop with <strong>non-parametric models</strong>, which don’t require to be optimized. Specifically, we will try to <strong>use parametric meta learners that produce non-parametric learners</strong>.</p>

<div>
<figure class="figure col-sm-5 float-right">
    <img src="/assets/img/blog/cs330/6/parametric_example.png" class="img-fluid" alt="Alt text." />
</figure>

<p>One benefit of non-parametric methods is that they generally work well in low data regimes, making it a great 
opportunity for few-shot learning problems at meta-test time. Nevertheless, during meta-training time, we would still 
like to use a parametric learner to exploit the potentially large amounts of data.</p>
</div>

<div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/6/l2_loss_example.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Comparison between $\ell_2$-distances of two augmented images to the original.</figcaption>
</figure>

<p>The key idea behind non-parametric approaches is to compare the task-specific test data to the data in the train 
dataset. We will continue using the example of the few-shot image classification problem, as in the previous posts.
If you want to compare images to each other, you need to come up with a certain <b>metric</b> to do so.</p>
</div>

<p>The simplest idea might be to utilize the $\ell_2$-distance. Unfortunately, it is not that simple. If you look at the
figure above, you can see the image of a woman on the right and two augmented versions on the left. When you calculate 
the $\ell_2$-distance between the original image and the augmented image, the distance between the blurry image and the
original one is smaller than the other distortion, even though this may resemble the original image more. For this 
problem, you could use a different metric, such as a <em>perceptual loss function</em> <d-cite key="johnson2016perceptual"></d-cite>, but in general, it might be
worthwhile to learn the metric from the data.</p>

<p>In this post, we will discuss three different ways of doing metric learning, starting with the easiest and building our way up. Firstly, we will talk about the most basic model: Siamese networks.</p>

<h3 id="siamese-networks">Siamese networks</h3>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/6/siamese_network.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Example architecture of a Siamese network. It takes two images as an input and outputs a binary label whether they belong to the same class or not.</figcaption>
</figure>

<p>With a Siamese network, the goal is to learn whether two images belong to the same class or not. The input to the model is two images, and it outputs whether it thinks they belong to the same class. However, the penultimate activations correspond to a learned distance metric between these images. At meta-train time, you are simply trying to minimize the binary cross-entropy loss.</p>

<p>At meta-test time, you need to compare the test image $x_\mathrm{test}$ against every image in the test-time training dataset $\mathcal{D}^\mathrm{tr}$, and then select the class of the image that has the highest probability. This corresponds to the equation below (for simplicity of the equation, we assume that only one sample will have $f_\theta(x_j^\mathrm{test}, x_k) &gt;  0.5$). Furthermore, $1$ corresponds to the indicator function.</p>

\[\hat{y}_j^\mathrm{test} := \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}} 1(f_\theta(x_j^\mathrm{test}, x_k) &gt;  0.5)y_k\;.\]

<p>With this method, there is a mismatch between meta-training and meta-testing. During meta-training, you are solving a binary classification problem, whilst during meta-testing, you are solving an $N$-way classification problem. You cannot phrase meta-training in the same way, since the indicator function $1$ makes $\hat{y}_j^\mathrm{test}$ non-differentiable. We will try to resolve this by introducing <strong>matching networks</strong>.</p>

<h3 id="matching-networks">Matching networks</h3>

<p>In the previous equation above, we saw that at meta-test time, we use the class of the most similar training example as the estimate of the test sample. In order to get rid of the mismatch between meta-training and meta-testing, we can rephrase the meta-testing objective similarly to what we saw. Let’s say we instead modify the procedure to use a mix of class predictions as a class estimate. This would result in the equation below.</p>

\[\hat{y}_j^\mathrm{test} := \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}} f_\theta(x_j^\mathrm{test}, x_k)y_k\;.\]

<p>At meta-train time, we can now use the same objective, and backpropagate through the cross-entropy loss $y_j^\mathrm{test} \log(\hat{y}_j^\mathrm{test}) + (1-y_j^\mathrm{test})\log(1-\hat{y}_j^\mathrm{test})$. This way, both meta-training and meta-testing are aligned with the same procedure. Our meta-training process would become:</p>

<ol>
  <li>Sample task $\mathcal{T}_i$.</li>
  <li>Sample two images per class, giving $D_i^\mathrm{tr}, D_i^\mathrm{test}$.</li>
  <li>
    <p>Compute $\hat{y}^\mathrm{test} = \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}_i} f_\theta(x^\mathrm{test}, x_k)y_k$.</p>
  </li>
  <li>Backpropagate the loss with respect to $\theta$.</li>
</ol>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/6/matching_network.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Example architecture of a matching network. It takes the entire training dataset as an input with a testing image and predicts the most likely class for the testing image given the training data.</figcaption>
</figure>

<p>This idea corresponds to so-called “matching networks” <d-cite key="vinyals2016matching"></d-cite>. Here, we embed each training image into some latent
space using a bidirectional LSTM $g_\theta$. Then, we encode the test image using a shared convolutional encoder 
$h_\theta$ and perform the dot product between the latent training vectors and the latent test vector, resulting in
$f_\theta(x^\mathrm{ts}, x_k)$. Finally, we take the dot products with the labels to obtain the prediction 
$\hat{y}^\mathrm{ts}.$ This way meta-training and meta-testing match, which resulted in a better performance than 
something like Siamese networks.</p>

<p>Let’s stand still with what we’re doing for a second and think about how this approach is non-parametric. If we recall 
from parametric models, we would always compute task-specific parameters $\phi_i \leftarrow f_\theta(\mathcal{D}_i^\mathrm{tr})$. 
However, now have integrated the parameters $\phi$ out by computing 
$\hat{y}_j^\mathrm{test} := \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}} f_\theta(x_j^\mathrm{test}, x_k)y_k$ directly 
by comparing to the training dataset, making it non-parametric.</p>

<div>
<figure class="figure col-sm-4 float-right">
    <img src="/assets/img/blog/cs330/6/matching_network_disadvantage.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Example of a disadvantage of a matching network.</figcaption>
</figure>

<p>In the meta-training procedure described, we would sample two images per class. But what would happen if we sampled
more than two images (ignoring potential class imbalance)? Well, with matching networks, each sample of each class is
evaluated independently with $f_\theta(x_j^\mathrm{test}, x_k)$ instead of together. This could lead to strange results
if the majority of a class has a low confidence but there is an outlier with a high confidence, overpowering the correct
label. This can be depicted in the right figure. Imagine if you want to predict the label of the black square. The 
dot-product score with the red sample might be so high that it overpowers the other samples, even though it is more 
likely part of the blue class. We will try to resolve this by calculating prototypical embeddings that average class 
information.</p>
</div>

<h3 id="prototypical-models">Prototypical models</h3>

<div>
<figure class="figure col-sm-5 float-right">
    <img src="/assets/img/blog/cs330/6/proto_model_example.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Example of a prototype network, which uses aggregation over the embeddings of each class.</figcaption>
</figure>

<p>Prototypical models <d-cite key="snell2017prototypical"></d-cite> will work quite similarly to what we have previously seen, but try to aggregate class 
information in order to prevent outliers. The figure on the right depicts this. Formally, we introduce class prototypes
$c_n = \frac{1}{K} \sum_{(x,y)\in\mathcal{D}_i^\mathrm{tr}} 1(y_k=n)f_\theta(x_k)$. After we compute these class-averaged
embeddings, a model will try to estimate the class of the test point by using something like Softmax probability, 
resulting in the equation below, where $d$ was the Euclidean or Cosine distance. Nevertheless, it could even be a learned
network as we have previously seen.</p>
</div>

\[p_\theta(y=n|x) = \frac{\exp(-d(f_\theta(x), c_n))}{\sum_{n^\prime}\exp(-d(f_\theta(x), c_{n^\prime}))}\;.\]

<p>As opposed to the matching networks, we are now using the same embedding function $f_\theta$ for both the training and testing datapoints.</p>

<h3 id="more-advanced-models">More advanced models</h3>

<p>The models that we talked about today are already quite expressive for non-parametric meta learning, but they all do <strong>some form of embedding followed by nearest-neighbours</strong>. However, sometimes you might need to reason about more complex relationships between datapoints. Let’s briefly discuss a few more recent works that approach this problem.</p>

<ol>
  <li><strong>Relation networks</strong> <d-cite key="sung2018learning"></d-cite>.
    <div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/6/relation_net.png" class="img-fluid" alt="Alt text." />
</figure>
   
<p>The idea is to learn non-linear relation modules on the embedding. They first embed the images and then compute this
relation score, which corresponds to the distance function $d$ that we saw with prototypical models.</p>
</div>
  </li>
  <li><strong>Infinite mixture of prototypes</strong> <d-cite key="allen2019infinite"></d-cite>.
    <div>
<figure class="figure col-sm-3 float-right">
    <img src="/assets/img/blog/cs330/6/mixture_of_prots.png" class="img-fluid" alt="Alt text." />
</figure>

<p>The idea is to learn an infinite mixture of prototypes, which is useful when classes are not easy to cluster 
nicely. For example, some breeds of cats might look similar to dogs, which would not be good when averaging class 
embeddings. In this case, we can have multiple prototypes per class.</p>
</div>
  </li>
  <li><strong>Graph neural networks</strong> <d-cite key="garcia2017few"></d-cite>.
    <div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/6/message_passing_npm.png" class="img-fluid" alt="Alt text." />
</figure>

<p>The idea is to do message passing on the embeddings instead of doing something as simple as nearest neighbours. 
This way, you can figure out relationships between different examples (i.e. by learning edge weights), and do more 
complex aggregation.</p>
</div>
  </li>
</ol>

<h2 id="properties-of-meta-learning-algorithms">Properties of meta-learning algorithms</h2>

<p>Now that we have seen all three different types of meta learning algorithms, we can compare each approach to see which
problems might benefit from which approach. Let’s first quickly summarize all approaches.</p>

<div class="row mt-3">
   <div class="col-sm mt-3 mt-md-0">
      <p><b>Black-box meta learning.</b></p>
      <figure class="figure col-sm-6">
         <img src="/assets/img/blog/cs330/6/bbml_model_small.png" class="img-fluid" alt="Alt text." />
      </figure>

      <p>$y^\mathrm{ts} = f_\theta(\mathcal{D}_i^\mathrm{tr}, x^\mathrm{ts})$.</p>
   </div>
   <div class="col-sm mt-3 mt-md-0">
      <p><b>Optimization-based meta learning.</b></p>
      
      <p>$y^\mathrm{ts} = f_\mathrm{MAML}(\mathcal{D}_i^\mathrm{tr}, x^\mathrm{ts}) = f_{\phi_i}(x^\mathrm{ts})$, where $\phi_i = \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}^\mathrm{tr})$.</p>
   </div>
   <div class="col-sm mt-3 mt-md-0">
      <p><b>Non-parametric meta learning.</b></p>
      
      <p>$y^\mathrm{ts} = f_\mathrm{PN}(\mathcal{D}_i^\mathrm{tr}, x^\mathrm{ts}) = \mathrm{softmax}(-d(f_\theta(x^\mathrm{ts}), c_n))$, where $c_n = \frac{1}{K} \sum_{(x,y)\in\mathcal{D}_i^\mathrm{tr}} 1(y_k=n)f_\theta(x_k)$.</p>
   </div>
</div>

<p>As you can see, all these methods share this perspective of a computational graph that we discussed in earlier posts. You can easily mix-and-match different components of these computation graphs. Below are some examples of paper that try this:</p>

<ol>
  <li>Gradient descent on relation network embeddings.</li>
  <li>Both condition on data and run gradient descent <d-cite key="rusu2018meta"></d-cite>.</li>
  <li>Model-agnostic meta learning, but initialize last layer as a prototype network during meta-training <d-cite key="triantafillou2019meta"></d-cite>.</li>
</ol>

<p>Let’s make a table of the benefits and downsides of each method that we have discussed up to this point in the series:</p>

<table>
  <thead>
    <tr>
      <th>Black-box</th>
      <th>Optimization-based</th>
      <th>Non-parametric</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>[+] Complete expressive power</td>
      <td>[~] Expressive for very deep models (in a supervised learning setting)</td>
      <td>[+] Expressive for most architectures</td>
    </tr>
    <tr>
      <td>[-] Not consistent</td>
      <td>[+] Consistent, reduces to gradient descent</td>
      <td>[~] Consistent under certain conditions</td>
    </tr>
    <tr>
      <td>[+] Easy to combine with a variety of learning problems</td>
      <td>[+] Positive inductive bias at the start of meta learning, handles varying and large number of classes well</td>
      <td>[+] Entirely feedforward, computationally fast and easy to optimize</td>
    </tr>
    <tr>
      <td>[-] Challenging optimization problem (no inductive bias at initialization)</td>
      <td>[-] Second-order optimization problem</td>
      <td>[-] Harder to generalize for varying number of classes</td>
    </tr>
    <tr>
      <td>[-] Often data-inefficient</td>
      <td>[-] Compute- and memory-intensive</td>
      <td>[-] So far limited to classification</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong>Expressive power</strong>: The ability of $f$ to model a range of learning procedures.</li>
  <li><strong>Consistency</strong>: Learned learning procedure will monotonically improve with more data</li>
  <li><strong>Uncertainty awareness</strong>: Ability to reason about ambiguity during learning.</li>
</ul>

<p>We have not yet discussed the uncertainty awareness of methods, but it plays an important role in active learning, 
calibrated uncertainty, reinforcement learning, and principled Bayes approaches. We will discuss this later on in the series!</p>

<h2 id="examples-of-meta-learning-in-practice">Examples of meta learning in practice</h2>

<p>In this section, we will very briefly talk about 6 different problem settings where meta learning has been used, some 
of which we have already seen in previous posts. This should give you a good idea of some different applications, and 
show you that it can be utilized in many different domains.</p>

<h3 id="land-cover-classification">Land-cover classification</h3>

<div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/6/paper_land_cover.png" class="img-fluid" alt="Alt text." />
</figure>

<p>The goal of this paper <d-cite key="russwurm2020meta"></d-cite> is to classify and segment satellite images in different regions of the world. Every region
corresponds to a task, and the datasets are thus images from a particular region. The problem is that manually segmenting
this data is expensive, so the authors use meta learning to quickly be able to segment new regions given limited training 
data on these regions.</p>

<p><b>Model</b>: Optimization-based (model-agnostic meta learning)</p>
</div>

<h3 id="student-feedback-generation">Student feedback generation</h3>

<div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/6/paper_student_feedback_results.png" class="img-fluid" alt="Alt text." />
</figure>

<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/6/paper_student_feedback_example.png" class="img-fluid" alt="Alt text." />
</figure>

<p>The goal of this paper <d-cite key="wu2021prototransformer"></d-cite> is to automatically provide students with feedback on coding assignments for high-quality Computer
Science education. The different tasks corresponded to different rubrics for different assignments or exams. The datasets
were then constructed of the solutions of the students (in this paper, they were always Python programs).</p>

<p><b>Supervised baseline</b>: Train a classifier per task, using same pre-trained CodeBERT <d-cite key="feng2020codebert"></d-cite>.</p>

<p>Outperforms supervised learning by 8-17%, and more accurate than human TA on held-out rubric! However, there is room
for improvement on a held-out exam.</p>

<p><b>Model</b>: Non-parametric (prototypical network with pre-trained Transformer, task information, and side information).</p>
</div>

<h3 id="low-resource-molecular-property-prediction">Low-resource molecular property prediction</h3>

<div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/6/paper_molecule_results.png" class="img-fluid" alt="Alt text." />
</figure>

<p>The goal of this paper <d-cite key="nguyen2020meta"></d-cite> is to predict certain chemical properties and activities of different molecules in Silico models,
which could potentially be useful for low-resolution drug discovery problems. The tasks here correspond to different
chemical properties and activations, and the corresponding datasets are different instances of these properties and
activations.</p>

<p><b>Model</b>: Optimization-based MAML, first-order MAML, and an ANIL Gated graph neural net base model.</p>
</div>

<h3 id="one-shot-imitation-learning">One-shot imitation learning</h3>

<div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/6/paper_imitation.png" class="img-fluid" alt="Alt text." />
</figure>

<p>The goal of this paper <d-cite key="yu2018one"></d-cite> is to do one-shot imitation learning for object manipulation by using video demonstrations of a
human. The tasks would be different manipulation problems. The training dataset would be the human demonstration, and the
testing dataset would be the tele-operated demonstration.</p>

<p><b>Note</b>: See that they training and testing datasets do not need to be sampled independently from the overall dataset 
for meta learning to work!</p>

<p><b>Model</b>: Model-agnostic meta learning with learned inner loss function.</p>
</div>

<h3 id="dermatological-image-classification">Dermatological image classification</h3>

<div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/6/paper_derm_res.png" class="img-fluid" alt="Alt text." />
</figure>

<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/6/paper_derm.png" class="img-fluid" alt="Alt text." />
</figure>

<p>The goal of this paper <d-cite key="prabhuprototypical"></d-cite> is to perform dermatological image classification that is good for all different skin conditions, 
which are the tasks in this case. The datasets consist of images of these skin conditions from different people.</p>

<p><b>Model</b>: Non-parametric prototype networks with multiple prototypes per class using clustering objective.</p>

<p>Results show that the clustering prototype networks perform better than normal ones and competitive against a ResNet 
model that is pre-trained on ImageNet and fine-tuned on 200 classes with balancing. This is a very strong baseline with
access to more info during training, and it requires re-training for new classes.</p>
</div>

<h3 id="few-shot-human-motion-prediction">Few-shot human motion prediction</h3>

<div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/6/paper_motion.png" class="img-fluid" alt="Alt text." />
</figure>

<p>The goal of this paper <d-cite key="gui2018few"></d-cite> is to do few-shot motion prediction using meta learning, which could potentially be useful for 
autonomous driving and human-robot interaction. The tasks are different humans and different motivation. The corresponding
train dataset $\mathcal{D}^\mathrm{tr}_i$ is composed of the past $K$ seconds of the motion, and the test set 
$\mathcal{D}^\mathrm{test}_i$ is composed of the future second(s) of the motion.</p>

<p><b>Note</b>: See that they training and testing datasets do not need to be sampled independently from the overall dataset 
for meta learning to work!</p>

<p><b>Model</b>: Optimization-based/black-box hybrid, MAML with additional learned update rule and a recurrent neural net base model.</p>
</div>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning" /><category term="course" /><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to to understand the third form of meta learning: non-parametric few-shot learning. We will also compare the three different methods of meta learning. Finally, we give practical examples of meta learning, in domains such as imitation learning, drug discovery, motion prediction, and language generation!]]></summary></entry><entry><title type="html">CS-330 Lecture 4: Optimization-Based Meta-Learning</title><link href="http://localhost:4000/blog/2024/cs330-stanford-obml/" rel="alternate" type="text/html" title="CS-330 Lecture 4: Optimization-Based Meta-Learning" /><published>2024-03-10T00:00:00+01:00</published><updated>2024-03-10T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/cs330-stanford-obml</id><content type="html" xml:base="http://localhost:4000/blog/2024/cs330-stanford-obml/"><![CDATA[<p>The goal of this lecture is to understand the <strong>basics of optimization-based meta learning</strong> techniques. You will also
learn about the <strong>trade-offs</strong> between black-box and optimization-based meta learning! If you missed the previous
lecture, which was about black-box meta learning and in-context learning with GPT-3, you can head over <a href="/blog/2024/cs330-stanford-bbml-icl/">here</a> to view it.</p>

<p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow 
of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p>

<p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_optbased_metalearning_2023.pdf">here</a>.</p>

<h2 id="overall-approach">Overall approach</h2>

<p>In the previous post, we looked into black-box meta learning. To recap, this approach attempts to output task-specific 
parameters or contextual information with some meta-model. One major benefit of this approach is its <strong>expressiveness</strong>.
However, it also requires solving a challenging optimization problem, which is incredibly data-inefficient.</p>

<figure class="figure col-sm-12">
    <img src="/assets/img/blog/cs330/5/oml_approach.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Computation pipeline for optimization-based meta learning.</figcaption>
</figure>

<p>In this post, we will focus on <strong>optimized-based meta learning</strong>. The key idea behind this is to <strong>embed optimization</strong> 
inside the inner learning process. In the figure above, an example of this idea is depicted. With some initial model 
$f_\theta$, we will run gradient descent on the datapoints in $\mathcal{D}_i^\mathrm{tr}$ to produce the task-specific 
network with parameters $\phi_i$. In summary, the goal will be to find the model parameters $\theta$ such that optimizing
these parameters to specific tasks is as <strong>effective</strong> and <strong>efficient</strong> as possible.</p>

<div>
<figure class="figure col-sm-6 float-right">
    <img src="/assets/img/blog/cs330/5/fine_tune_loss.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Results on model fine-tuning with respect to dataset size.</figcaption>
</figure>

<p>As recalled from a previous post, we are trying to do a similar thing in fine-tuning. Specifically, in fine-tuning, we 
are using a pre-trained model with parameters $\theta$ to find new task-specific parameters 
$\phi \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}^\mathrm{tr})$. We also saw that 
fine-tuning often performed much better than training from scratch. However, fine-tuning generally needs a lot of data 
in order to adapt well to a new task, meaning it doesn’t perform well with few-shot learning.</p>
</div>

<p>You may now see that our proposed optimization-based meta learning approach tries to optimize pre-trained parameters
$\theta$, such that it does well in the few-shot regime, unlike transfer learning through fine-tuning. If we now adapt
the meta learning objective function from the previous lecture with this fine-tuning, we obtain the following:</p>

\[\min_\theta \sum_{\mathcal{T}_i}\mathcal{L}(\theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i), \mathcal{D}^\mathrm{test}_i)\;.\]

<p>As you can see in the equation, you are trying to find the parameters $\theta$, such that performing fine-tuning 
$\theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)$ on a dataset $\mathcal{D}^\mathrm{tr}_i$ 
of a task $\mathcal{T}_i$ performs well on the task-specific test set $\mathcal{D}_i^\mathrm{test}$. If these datasets 
are small, the objective function will still need to optimize pre-training the model for effective fine-tuning on these 
tasks, meaning that it might work in the few-shot regime.</p>

<p>We can write down the general training pipeline with the following steps:</p>

<ol>
  <li>Sample a task $\mathcal{T}_i$.</li>
  <li>Sample disjoint datasets $\mathcal{D}_i^\mathrm{tr}$ and $\mathcal{D}_i^\mathrm{test}$ from $\mathcal{D}_i$.</li>
  <li>Optimize $\phi_i \leftarrow \theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}_i^\mathrm{tr})$.</li>
  <li>Update $\theta$ using $\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$.</li>
</ol>

<p>Notice that we only optimize $\phi_i$ to then use it to update parameters $\theta$. This means that $\phi_i$ is 
discarded and re-computed at each iteration. Additionally, we can optimize for different parameters, such as $\alpha$ 
as well by including it as a parameter in $\theta$. Besides computational efficiency of this, there is another problem.
If we expand $\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$, we get <strong>second-order</strong> derivates. Let’s 
show this below:</p>

\[\displaylines{\nabla_\theta\mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test}) \cr
= \nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\vert_{\bar{\phi} = \phi_i} \frac{\partial\phi_i}{\partial\theta} \cr
= \nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\vert_{\bar{\phi} = \phi_i}\left(I - \alpha \nabla^2_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)\right)\;.}\]

<p>Unfortunately, we can see the Hessian $\nabla^2_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)$. We would have to
compute this, but luckily, $\nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\vert_{\bar{\phi} = \phi_i}$
is a row vector. Due to Hessian-vector products, we can compute $\nabla_\theta\mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$ 
without computing the whole Hessian. Let’s show this below:</p>

\[\displaylines{g(x + \Delta x) \approxeq g(x) + H(x)\Delta x \cr
g(x + rv) \approxeq g(x) + rH(x)v \cr
H(x)v \approxeq \frac{g(x+rv) - g(x)}{r}\;.}\]

<p>In the first line, we are using a simple Taylor expansion to approximate the function $g(x+\Delta x)$, which is the 
gradient function. Then, we rewrite $\Delta x$ as with scalar $r$ and vector $v$. We can finally rearrange the equation
to get to our result. The last line shows that we can approximate a Hessian-vector product by using <strong>two gradient evaluations</strong>.
Whilst this is still more than one, it is much more computationally efficient than computing the entire Hessian matrix.</p>

<p>Since $\nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\vert_{\bar{\phi} = \phi_i}\left(I - \alpha \nabla^2_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)\right) = v -\alpha vH$, 
you can see that we can use the previous result to compute the gradient, rather then computing the whole Hessian.</p>

<p>A common misconception is that you will get higher than 2nd-order derivatives when computing multiple iterations of 
$\phi_i \leftarrow \theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}_i^\mathrm{tr})$. However, this is not 
true, since the derivatives will be sequential rather than nested. You can try this yourself if you want to test your 
knowledge! If you do this correctly, you will see that doing more iterations will increase the amount of memory used 
<strong>linearly</strong>, and the amount of compute necessary <strong>linearly</strong> as well. In practice, usually you do not need more than
5 inner gradient steps, which is usually fine for few-shot learning tasks. In future posts, we will discuss methods that
work for hundreds of inner gradient steps.</p>

<div>
<figure class="figure col-sm-5 float-right">
    <img src="/assets/img/blog/cs330/5/maml_viz.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Visualization of the parameter space of MAML.</figcaption>
</figure>

<p>This approach, which is called <b>Model-Agnostic Meta-Learning</b><d-cite key="finn2017model"></d-cite> (MAML), is also represented in the figure 
on the right. Here, $\phi_1^*, \phi_2^*, \phi_3^*$ are the optimal parameters for tasks $1, 2, 3$.</p>
</div>

<p>At meta-test time, we can follow the following steps, which are very similar to meta-training:</p>

<ol>
  <li>Sample a task $\mathcal{T}_i$.</li>
  <li>Given training dataset $\mathcal{D}_j^\mathrm{tr}$.</li>
  <li>Optimize $\phi_j \leftarrow \theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}_j^\mathrm{tr})$.</li>
  <li>Make predictions on new datapoints $f_{\phi_j}(x)$.</li>
</ol>

<hr />

<h2 id="compare-optimization-based-vs-black-box">Compare: optimization-based vs. black-box</h2>

<p>In the previous section, we got some intuition behind the Model-Agnostic Meta-Learning algorithm. For now, it looks 
like this method is completely different from black-box meta learning, but let’s spend some times trying to understand 
the connection between the two. Let’s write down both objectives again.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <p><b>Black-box meta learning</b></p>
        <p>$y^\mathrm{ts} = f_\mathrm{black-box}(\mathcal{D}^\mathrm{tr}_i, x^\mathrm{ts})$.</p>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <p><b>Model-agnostic meta learning</b> (MAML)</p>
        <p>$y^\mathrm{ts} = f_\mathrm{MAML}(\mathcal{D}^\mathrm{tr}_i, x^\mathrm{ts}) = f_{\phi_i}(x^\mathrm{ts})$, where
         $\phi_i = \theta - \alpha  \nabla_\theta \mathcal{L}(\theta ,\mathcal{D}_i^\mathrm{tr})$.</p>
    </div>
</div>

<p>If you look at both equations, both equations look more similar than it may seem. Both methods are still only functions of
$\mathcal{D}^\mathrm{tr}_i$ and $x^\mathrm{ts}$. The main difference is that MAML uses an <strong>“embedded” gradient operator</strong> 
in its <strong>computation graph</strong>, which is just a directed graph of all computations that is done by the function.</p>

<p>Keeping this idea of a computation graph in mind, you might think of the idea to mix and match different components of 
the computation graph. This idea has been explored in various works. For example, one paper learned the initialization 
parameters $\theta$, but replaced the gradient updates with a learned network<d-cite key="ravi2016optimization"></d-cite>, meaning that $\phi_i = \theta - f(\theta, \mathcal{D}_i^\mathrm{tr}, \nabla_\theta\mathcal{L})$. 
It turns out this approach is not very practical, but it has a nice conceptual meaning that goes along well with the 
idea of a computation graph.</p>

<h3 id="comparing-performances">Comparing performances</h3>

<p>We now wish to compare the two different approaches to each other. First, let’s think about the following: If we have a
test task $\mathcal{T}_j$ that is a bit different from the tasks that we trained our models on, which approach do you 
think will be better?</p>

<p>We hope to see that optimization-based meta learning performs better in this case, because it’s explicitly optimizing
the effectiveness of fine-tuning to new tasks. With black-box meta learning, we are essentially just training a model to
output model parameters or context, which does not guarantee to work for unseen tasks (though we hope it will!).</p>

<figure class="figure col-sm-12">
    <img src="/assets/img/blog/cs330/5/adaptation_res.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Results of different meta learning algorithms on augmented samples from the Omniglot dataset.</figcaption>
</figure>

<p>We will test this by looking at the Omniglot image classification problem again. However, we will try to see what 
happens when we vary the tasks after training the models on them. In this case, digits from the datasets are warped 
to see a form of out-of-distribution performance. In the figure above, which is taken from<d-cite key="finn2017meta"></d-cite>, you can see that,
as expected, an optimization-based method like MAML performs much better than black-box-based approaches like SNAIL <d-cite key="mishra2017simple"></d-cite>
or MetaNet <d-cite key="munkhdalai2017meta"></d-cite> for these types of problems.</p>

<p>You might think that this structure comes at a cost of expressiveness. But, in <d-cite key="finn2017meta"></d-cite>, they showed that MAML can 
approximate any function of $\mathcal{D}^\mathrm{tr}_i, x^\mathrm{ts}$ given some assumptions (i.e. you need a very
deep network for this to hold). However, despite these assumptions, this result shows that MAML benefits of the 
inductive bias of optimizing fine-tuning explicitly without losing any of the expressive power.</p>

<hr />

<h2 id="challenges-and-solutions">Challenges and solutions</h2>

<p>Whilst optimization-based meta learning methods may sound perfect from this post, there are quite some challenges when
using these methods. We will list some of them and discuss them in more detail.</p>

<ol>
  <li>
    <p><strong>Bi-level optimization can exhibit instabilities.</strong></p>

    <p>This can be the case because you have nested optimizations that are heavily dependent on each other. Some unexpected result in one level of the optimization can result in an unexpected effect, which will then be propagated throughout training. There are multiple simple tricks that can be used to try to stabilize training:</p>

    <ul>
      <li>Automatically learn inner vector learning rate, tune outer learning rate <d-cite key="li2017meta"></d-cite> <d-cite key="behl2019alpha"></d-cite>.</li>
      <li>Optimize only a subset of the parameters in the inner loop <d-cite key="zhou2018deep"></d-cite> <d-cite key="zintgraf2019fast"></d-cite>.</li>
      <li>Decouple inner learning rate, batch normalization statistics per-step <d-cite key="antoniou2018train"></d-cite>.</li>
      <li>Introduce context variables for increased expressive power <d-cite key="finn2017one"></d-cite> <d-cite key="zintgraf2019fast"></d-cite>.</li>
    </ul>
  </li>
  <li>
    <p><strong>Backpropagating through many inner gradient steps is compute- &amp; memory-intensive.</strong></p>

    <p>As we said before, the amount of inner gradient steps with MAML is usually only around 5, due to the amount of compute and memory required to optimize over such a computation graph. Again, there are some tricks to try to address this, but we will discuss more options in the lecture about large-scale meta learning.</p>

    <ul>
      <li>(Crudely) approximate $\frac{\delta\phi_i}{\delta\theta}$ as the identity matrix <d-cite key="nichol2018reptile"></d-cite>. This seems to work surprisingly well for simple few-shot learning problems, but it doesn’t scale to more complex problems.</li>
      <li>Only optimize the last layer of weights using ridge regression, logistic regression <d-cite key="bertinetto2018meta"></d-cite>, or support vector machines <d-cite key="lee2019meta"></d-cite>. This can lead to a closed form convex optimization problem on top of the meta-learned features.</li>
      <li>Derive the meta-gradient using the implicit function theorem <d-cite key="rajeswaran2019meta"></d-cite>. This way you can compute the full gradient without differentiating through the entire computation graph of the multiple iterations.</li>
    </ul>
  </li>
</ol>

<p>Let’s summarize the upsides and downsides:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
      <p><b>Upsides</b></p>
      <ul>
         <li>Positive inductive bias at the start of meta learning.</li>
         <li>Extrapolates better via structure of optimization.</li>
         <li>Maximally expressive for a sufficiently deep network.</li>
         <li>Model-agnostic!</li>
      </ul>
    </div>
    <div class="col-sm mt-3 mt-md-0">
      <p><b>Downsides</b></p>
      <ul>
         <li>Typically requires second-order optimization.</li>
         <li>Can be compute and/or memory intensive.</li>
         <li>Can be prohibitively expensive for large models.</li>
      </ul>
    </div>
</div>

<hr />

<h2 id="case-study-of-land-cover-classification">Case study of land cover classification</h2>

<p>We will now study an example of optimization-based meta learning for a pretty cool problem: land cover classification <d-cite key="russwurm2020meta"></d-cite>! 
Imagine that you are given a bunch of <strong>satellite data of different terrains</strong>, and your goal is to predict <strong>what the land is used for</strong>
by segmenting it into different classes. Research like this can for example be used to understand how different climates 
change over time, or urban planning. Unfortunately, it is very expensive to label these images, leading to small datasets.</p>

<figure class="figure col-sm-12">
    <img src="/assets/img/blog/cs330/5/landmark_problem.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Depiction of the use of MAML for land cover classification.</figcaption>
</figure>

<p>As you can see in the image above, we have terrains from different regions of the world, which can look very different, 
but probably still share some geological structural similarities. Given this description, try to think to yourself: Do 
you think meta learning would be a good approach to this problem?</p>

<p>If we let our tasks correspond to these different regions, we can try to use optimization-based meta learning to learn 
to effectively and efficiently fine-tune to unknown regions of the world! For these new regions, we would manually 
segment small amount of data, and then use our model to take care of the rest.</p>

<p>The paper looks at two datasets:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
         <p>SEN12MS <d-cite key="schmitt2019sen12ms"></d-cite>, which contains geographic metadata, making it easy to construct good meta-train, meta-val, and meta-test sets.</p>
         <figure class="figure col-sm-12">
            <img src="/assets/img/blog/cs330/5/sen12ms.png" class="img-fluid" alt="Alt text." />
            <figcaption class="figure-caption text-center">Example of data in the SEN12MS dataset.</figcaption>
         </figure>
    </div>
    <div class="col-sm mt-3 mt-md-0">
         <p>DeepGlobe <d-cite key="demir2018deepglobe"></d-cite>, which did not provide this metadata. Instead, clustering was used to create datasets of similar terrains.</p>
         <figure class="figure col-sm-12">
            <img src="/assets/img/blog/cs330/5/deepglobe.png" class="img-fluid" alt="Alt text." />
            <figcaption class="figure-caption text-center">Example of data in the DeepGlobe dataset.</figcaption>
         </figure>
    </div>
</div>

<figure class="figure col-sm-12">
    <img src="/assets/img/blog/cs330/5/use_case_res.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Results of different types of approaches to the land cover classification problem.</figcaption>
</figure>

<p>The results of using MAML on these two datasets is shown in the figure above. As you can see, MAML is more efficient 
and effective than (dark blue) pre-training on meta-training data and fine-tuning and (light blue) training on the new 
terrain from scratch. Hopefully these results give you a good idea of the potential of meta learning for few-shot 
learning!</p>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning" /><category term="course" /><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the basics of optimization-based meta learning techniques. You will also learn about the trade-offs between black-box and optimization-based meta learning!]]></summary></entry><entry><title type="html">CS-330 Lecture 3: Black-Box Meta-Learning &amp;amp; In-Context Learning</title><link href="http://localhost:4000/blog/2024/cs330-stanford-bbml-icl/" rel="alternate" type="text/html" title="CS-330 Lecture 3: Black-Box Meta-Learning &amp;amp; In-Context Learning" /><published>2024-03-03T00:00:00+01:00</published><updated>2024-03-03T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/cs330-stanford-bbml-icl</id><content type="html" xml:base="http://localhost:4000/blog/2024/cs330-stanford-bbml-icl/"><![CDATA[<p>The goal of this lecture is to learn how to <strong>implement black-box meta-learning</strong> techniques. We will also talk about a
<strong>case study of GPT-3</strong>! If you missed the previous lecture, which was about transfer learning by fine-tuning and meta
learning, you can head over <a href="/blog/2024/cs330-stanford-tl-ml/">here</a> to view it.</p>

<p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow 
of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p>

<p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_metalearning_bbox_2023.pdf">here</a>.</p>

<h2 id="black-box-adaptation-approaches">Black-box adaptation approaches</h2>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/omniglot.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Example of the Omniglot dataset.</figcaption>
</figure>

<p>The content of this section will build on the general recipe for meta-learning problems that we saw in the previous 
lecture. In order to explain it, we will use the example of the Omniglot dataset <d-cite key="lake2019omniglot"></d-cite>, which is a dataset of 1,623 
characters from 50 different alphabets. In this problem, every alphabet would refer to a different task. In our example,
we will do 3-way 1-shot learning, meaning that our sampled datasets consist of 3 classes with 1 example per class at 
every step. One iteration of the black-box meta-training process then has the following steps:</p>

<ol>
  <li>Sample task $\mathcal{T}_i$ or a mini-batch of tasks. In our case, this would correspond to generating the language(s).</li>
  <li>From the selected language(s), we sample disjoint datasets $\mathcal{D}_i^\mathrm{tr}$ and $\mathcal{D}_i^\mathrm{test}$ from $\mathcal{D}_i$. In our example, this will be a disjoint dataset with 3 samples of characters for every language alphabet.</li>
</ol>

<div>
<figure class="figure col-sm-5 6 float-right">
    <img src="/assets/img/blog/cs330/4/param_lstm.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Basic model architecture for black-box meta learning.</figcaption>
</figure>

<p>Now that we have these datasets, our goal is to train a neural network to represent $\phi_i = f_\theta(\mathcal{D}_i^\mathrm{tr})$. 
After computing these task parameters given a sampled training dataset, we can predict the test targets with $y^\mathrm{ts} = g_{\phi_i}(x^\mathrm{ts})$. 
An example of how such a model could work, is depicted in the figure above. Here, we are using a sequence model for 
$f_\theta$, which generates the parameters $\phi_i$. However, you can use all your fancy architectures that can handle 
a varying number of input sample. This is necessary due to varying dataset lengths.</p>
</div>

<p>After computing $y^\mathrm{ts}$, we can do backpropagation of the loss that is generated with the this test dataset. 
The full optimization objective is shown in the equation below:</p>

\[\min_\theta \sum_{\mathcal{T}_i} \sum_{(x,y) \sim \mathcal{D}^\mathrm{test}_i} - \log g_{\phi_i}(y\vert x) = \min_\theta \sum_{\mathcal{T}_i}\mathcal{L}(f_\theta(\mathcal{D}^\mathrm{tr}_i), \mathcal{D}^\mathrm{test}_i)\;.\]

<p>Notice that we are optimizing the parameters $\theta$. The task-specific parameters $\phi_i$ are generated by 
$f_\theta(\mathcal{D}_i^\mathrm{tr})$, and so they are not updated. Also note that the loss is calculated with 
respect to the sampled <strong>test dataset</strong>! This is no problem, since it makes sense to evaluate on new tasks for meta 
learning.</p>

<p>Now that you understand the architecture, we can write down the last two steps of the meta-training process:</p>

<ol>
  <li>Compute $\phi_i \leftarrow f_\theta(\mathcal{D}_i^\mathrm{tr})$.</li>
  <li>Update $\theta$ using $\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$.</li>
</ol>

<h3 id="a-more-scalable-architecture">A more scalable architecture</h3>

<p>However, we run into an issue. How do we let the model $f_\theta$ output another model’s parameters $\phi_i$? Not only 
can this be quite tricky to do, it also does not scale to larger parameter vectors $\phi_i$! Can you think of an 
alternative way of going this?</p>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/better_model.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">More scalable architecture for black-box meta-learning.</figcaption>
</figure>

<p>Instead of letting $f_\theta$ output $\phi_i$, we instead output a hidden state $h_i$, which is a low-dimensional 
vector that is supposed to represent contextual task information from the training dataset. If you recall the different
ways of conditioning that we saw for multi-task learning, you can see that we can train a model end-to-end by 
conditioning as $y^\mathrm{ts} = g_{\phi}(x^\mathrm{ts} \vert h_i)$. Now, notice that we have a general set of 
parameters $\phi$ for $g$; it does not need to be task-specific anymore, since we are already conditioning on task
information. In the figure above, $\theta$ are the parameters of the sequence model, and $\phi$ are the parameters of 
the convolutional network.</p>

<p>❗One problem that sometimes occurs with this architecture, is that the model learns to <strong>ignore conditioning on $h_i$.</strong>
In that case, it is essentially just learning to memorize, and not using the training dataset. In order to avoid that,
you can randomize the numerical label assignment to the target variables when sampling the datasets $\mathcal{D}^{tr}_i$
and $\mathcal{D}^{test}_i$. If the numerical label is different each time, it cannot just memorize the sample from the 
testing set.</p>

<h3 id="black-box-adaptation-architectures">Black-box adaptation architectures</h3>

<p>The architecture that we just presented was more-or-less first proposed on the Omniglot dataset at ICML in 2016 <d-cite key="santoro2016meta"></d-cite>.
It used LSTMs with Neural Turing Machines (which are not used anymore nowadays). Since then, a lot of new architectures 
have been proposed.</p>

<p>At ICML 2018, an architecture called the DeepSet architecture <d-cite key="garnelo2018conditional"></d-cite> was published. The idea is to pass all your dataset 
samples through a feedforward neural network to get an embedding of each sample, and then average those. This way, you have
a permutation-invariant model which is still model-agnostic. Given some conditions on the width and depth of the network, 
these models can represent any permutation-invariant function.</p>

<p>There are quite some more papers that used other external memory mechanisms <d-cite key="munkhdalai2017meta"></d-cite>, or convolutions 
and attention <d-cite key="mishra2017simple"></d-cite>.</p>

<p>Unfortunately, these models are still quite limited in capabilities against “difficult” datasets, as you can see in the
table below.</p>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/bmml_results.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Results of a model trained with black-box meta-learning.</figcaption>
</figure>

<p>In summary, some benefits of black-box meta learning are its <strong>expressiveness</strong>, how easy it is to combine with a 
<strong>variety of learning problems</strong> (such as SL or RL). Nonetheless, it is a <strong>challenging optimization problem</strong> for a 
<strong>complex model</strong>, and it is often <strong>data-inefficient</strong>.</p>

<hr />

<h2 id="case-study-of-gpt-3">Case study of GPT-3</h2>

<p>With the rise of research on in-context learning, especially with foundation models, GPT-3 <d-cite key="brown2020language"></d-cite> is a good example of 
a black-box meta-learner, trained on language generation tasks. We can represent the task-specific datasets 
$\mathcal{D}_i^\mathrm{tr}$ as a sequence of characters, and $\mathcal{D}_i^\mathrm{test}$ as the following sequence 
of characters. This way, $\mathcal{D}_i^\mathrm{tr}$ is what the model is being conditioned on (its context), and 
$\mathcal{D}_i^\mathrm{test}$ is what it has to generate.</p>

<p>The meta-training dataset consists of crawled data from the internet, English-language Wikipedia, and two books corpora,
with a giant Transformer architecture as its network (175 billion parameters, 96 layers, 3.2M batch size).</p>

<p>For these datasets, there are a multitude of different tasks such as, but definitely not limited to, 
<strong>spelling correction</strong>, <strong>simple math problems</strong>, or <strong>translating between languages</strong>. By encoding every task as text,
the authors are able to obtain meta-training data incredibly easily.</p>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/4/gpt3_pipeline.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Abstract representation of the training meta-train pipeline of GPT-3.</figcaption>
</figure>

<p>In the case of GPT-3, text generation, also known as in-context learning, represents the inner loop of the optimization 
process. The outer loop represents the model optimizing across different tasks, which is very similar to the process that
we saw in the previous section.</p>

<p>With this model, you can easily do few-shot learning by adding examples in text form to the context of the model. Even 
through the model is far from perfect, its results are extremely impressive. It is also no oracle and can fail in 
unintuitive ways! If there is anything we have learned from recent research, it is that <strong>the choice of $\mathcal{D}^\mathrm{tr}$ 
at test time matters</strong> (welcome to the world of prompt engineering).</p>

<p>It is also interesting to think about what is needed for <strong>few-shot learning to emerge</strong> when training a model. This is 
an active research are, but it seems that (1) temporal correlation in your data with dynamic meaning of words, and (2) 
large model capacities definitely seems to make a difference here <d-cite key="chan2022data"></d-cite>.</p>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning" /><category term="course" /><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to implement black-box meta-learning techniques. We will also talk about a case study of GPT-3!]]></summary></entry><entry><title type="html">CS-330 Lecture 2: Transfer Learning and Meta-Learning</title><link href="http://localhost:4000/blog/2024/cs330-stanford-tl-ml/" rel="alternate" type="text/html" title="CS-330 Lecture 2: Transfer Learning and Meta-Learning" /><published>2024-03-03T00:00:00+01:00</published><updated>2024-03-03T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/cs330-stanford-tl-ml</id><content type="html" xml:base="http://localhost:4000/blog/2024/cs330-stanford-tl-ml/"><![CDATA[<p>The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two 
tasks to share a common structure, and start thinking about meta learning. If you missed the previous lecture, which was
about multi-task learning, you can head over <a href="/blog/2024/cs330-stanford-mtl/">here</a> to view it.</p>

<p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow 
of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p>

<p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_finetune_transfer_meta_learning_problem_setup_2023.pdf">here</a>.</p>

<h2 id="transfer-learning">Transfer learning</h2>

<p>In contrast to multi-task learning, which tackles several tasks $\mathcal{T}_1, \cdots, \mathcal{T}_i$ simultaneously, 
transfer learning takes a sequential approach. It focuses on mastering a specific task $\mathcal{T}_b$ after the 
knowledge has been acquired from source task(s) $\mathcal{T}_a$. A common assumption is that $\mathcal{D}_a$ cannot be 
accessed during the transfer.</p>

<p>Transfer learning is a valid solution to multi-task learning, because it can sequentially apply knowledge from one task 
to another. This is unlike multi-task learning, which requires simultaneous learning of all tasks.</p>

<p>It is advantageous in the case of a large dataset $\mathcal{D}_a$, where continuous retraining is not feasible. Transfer
learning makes sense here by utilizing the acquired knowledge without the need for repetitive training on the large
dataset. Additionally, when you do not need to train two tasks simultaneously, you might opt to solve it them 
sequentially using transfer learning.</p>

<h3 id="transfer-learning-through-fine-tuning">Transfer learning through fine-tuning</h3>

\[\phi \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}_\mathrm{tr})\;.\]

<p>One method of transfer learning involves the fine-tuning of a pre-trained model with parameters $\theta$. This process 
starts with a model whose parameters have been <b>initially trained on a large, diverse dataset</b>, such as ImageNet <d-cite key="huh2016makes"></d-cite>.
The usefulness of fine-tuning lies in its ability to adapt these pre-trained parameters to a new task $\mathcal{T}_b$ by
continuing the training process with a dataset $\mathcal{D}_\mathrm{tr}$ specific to that task. Typically, this involves 
many iterations of gradient descent steps, where the pre-trained model's parameters $\theta$ are updated by moving in the
direction that minimizes the loss $\mathcal{L}$. This optimization process is depicted in the equation above.</p>

<p>When utilizing fine-tuning for transfer learning, there are several common design choices that are usually considered:</p>

<ul>
  <li>Opting for a <strong>lower learning rate</strong> to prevent the overwriting of the knowledge captured during pre-training.</li>
  <li>Employing even <strong>smaller learning rates for the earlier layers</strong> of the network, preserving more generic features.</li>
  <li>Initially <strong>freezing the early layers</strong> of the model, then gradually unfreezing them as training progresses.</li>
  <li><strong>Reinitializing the last layer</strong> to tailor it to the specifics of the new task.</li>
  <li><strong>Searching over hyperparameters</strong> using cross-validation to find the optimal configuration.</li>
  <li>Making smart choices about the <strong>model architecture</strong>.</li>
</ul>

<figure class="figure col-sm-11 float-right">
    <img src="/assets/img/blog/cs330/3/pretraining_datasets.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center"> Aggregate performance of a model across 10 finetuning datasets when it is (i) randomly 
    initialized (ii) pretrained on upstream corpus (BookWiki) (iii) pretrained on the finetuning dataset itself.</figcaption>
</figure>

<p>❗However, this common knowledge does not always hold true. For example, when using unsupervised pre-trained objectives,
you may not require diverse data for pre-training. The figure above shows that a model that was pretrained on the 
downstream dataset performs similarly to an upstream corpus such as BookWiki <d-cite key="krishna2022downstream"></d-cite>.</p>

<figure class="figure col-sm-11 float-right">
    <img src="/assets/img/blog/cs330/3/layer_tuning.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center"> How fine-tuning different layers has different effects.</figcaption>
</figure>

<p>Furthermore, depending on the downstream task, it may be better to tune the first or middle layers, rather than the last
layers. For example, for image corruption, it makes more sense to fine-tune the first layer of the model, since it’s
more of an input-level shift in data distribution <d-cite key="lee2022surgical"></d-cite>. The figure below shows more of these examples. Chelsea’s 
advice is to first <strong>train the last layer</strong>, and then <strong>fine-tune the entire network</strong> <d-cite key="kumar2022fine"></d-cite>, since fine-tuning can 
distort pre-trained features.</p>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/3/tl_dataset_size.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">The effect of fine-tuning with different dataset sizes.</figcaption>
</figure>

<p>However, one big disadvantage to fine-tuning is that <strong>it does not work well for very small target datasets</strong>. An 
example of this can be seen in the figure above. Luckily, this is where meta learning comes into play!</p>

<hr />

<h2 id="introduction-to-meta-learning">Introduction to meta learning</h2>

<p>With transfer learning, we initialize a model and then hope that it helps to solve the target task, by for example 
fine-tuning it. With meta-learning, we are asking the question of whether we can <strong>explicitly optimize for transferability</strong>.
Thus, given a set of training tasks, can we optimize the ability to learn these tasks quickly, so that we can learn <em>new</em> 
tasks quickly too.</p>

<p>When learning a task, we are very roughly mapping a task dataset to a set of model parameters through a function 
$\mathcal{D}^\mathrm{tr}_i \rightarrow \theta$. In meta learning, we are asking whether we can optimize this function
for a small $\mathcal{D}_i^\mathrm{tr}$.</p>

<p>There are two ways to view meta-learning algorithms:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <p><b>(1) Mechanistic view</b></p>
        <p>Construct a deep network that can read in an entire dataset and make predictions for new datapoints. Training this
        network uses a meta-dataset, which itself consists of many datasets, each for a different task.</p>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <p><b>(2) Probabilistic view</b></p>
        <p>Extract shared prior knowledge from a set of tasks that allows for efficient learning of new tasks. Then, learning a 
        new task uses this prior and a (small) training set to infer the most likely posterior parameters.</p>
    </div>
</div>

<h3 id="a-probabilistic-view-on-meta-learning">A probabilistic view on meta learning</h3>

<div>
<figure class="figure col-sm-5 float-right">
    <img src="/assets/img/blog/cs330/3/mtl_graphical_model.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Graphical model of multi-task- and meta-learning.</figcaption>
</figure>

<p>Expanding on the probabilistic (Bayesian) view of meta learning, let’s look at a graphical model for multi-task and
meta learning. To quickly recap what a graphical model represents, consider two random variables $X$ and $Y$. If there
is an arrow from $X$ to $Y$ in the graphical model, it means that $p(Y\vert X) \neq P(Y)$, meaning that the random 
variable $Y$ is dependent on $X$. Furthermore, you can nest certain variables (the rounded squared squares with $i$ 
and $j$) in the figure on the right, if you would like to repeat them for different indices.</p>
</div>

<p>Now we can start interpreting the graphical model for multi-task learning. Merging the training and testing sets, we 
immediately see that the target variable of datapoint $j$ for task $i$, denoted as $y_{i,j}$, is dependent on both the 
input data $x_{i,j}$ and the task-specific “true” parameter(s) $\phi_i$. The only difference between the training and 
testing data is that the target variables of the test dataset are not observed, whilst the others are all latent 
variables (unobserved). As we saw, for each task, we have task-specific true parameters $\phi_i$. However, if we share
some <strong>common structure</strong> between multiple tasks, we can condition these parameters on this common structure. Hence, 
$\theta$ defines the parameters related to the shared structure between different tasks.</p>

<p>This shared structure means that task parameters $\phi_{i_1}, \phi_{i_2}$ become independent when conditioning on the 
shared parameters $\theta$: $\phi_{i_1} \perp \phi_{i_2} \vert \theta$. Furthermore, the entropy of $p(\phi_i \vert \theta)$
is lower than $p(\phi_i)$, as there is less distributional noise from common structure.</p>

<p>Let’s now have a thought exercise. If we can identify $\theta$, then when should learning $\phi_i$ be faster than 
learning from scratch? Let’s think about one extreme. If the shared information fully describes the task-specific 
information, we would see that $p(\phi_i \vert \theta) = p(\phi_i \vert \phi_i) = 1$. From that, we can see that it is
faster if there is a lot of common information about the task that is captured by $\theta$. In a more general case, if 
the entropy $\mathcal{H}(p(\phi_i\vert\theta)) = 0$, meaning that you can predict $\phi_i$ with 100% accuracy given 
$\theta$, you can learn the fastest. However, this does not necessarily mean that $\phi_i = \theta$!</p>

<p>From all of this, we can define <strong>structure</strong> as a <strong>statistical dependence</strong> on <strong>shared latent information 
$\theta$.</strong> Let’s now see some examples of the type of information that $\theta$ may contain.</p>

<ul>
  <li>In a multi-task sinusoid problem, $\theta$ corresponds to the family of sinusoid functions, which is everything except the phase and amplitude.</li>
  <li>In a multi-language machine translation problem, $\theta$ corresponds to the family of all language pairs.</li>
</ul>

<p>Note that $\theta$ is <strong>narrower</strong> than the space of all possible functions!</p>

<p>We will discuss this probabilistic view on meta learning more in later lectures, but for the remainder of this lecture, 
we will switch back to the mechanistic view.</p>

<h3 id="how-does-meta-learning-work">How does meta learning work?</h3>

<figure class="figure col-sm-12 float-right">
    <img src="/assets/img/blog/cs330/3/ml_example.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Example of a meta-learning object classification problem.</figcaption>
</figure>

<p>Let’s consider an image classification problem, where you have different tasks. In the problem above, the different 
tasks contain different images to classify. For the (meta-)training process, we have tasks $\mathcal{T}_1, \mathcal{T}_2, \cdots, \mathcal{T}_n$
and we would like to do meta-testing on a new task $\mathcal{T}_\mathrm{test}$. The goal is to learn to solve task 
$\mathcal{T}_\mathrm{test}$ more quickly than from scratch. We can then test after training on the few examples from 
the new (in this case testing) task $\mathcal{T}_\mathrm{test}$. Of course, this problem settings generalizes to any 
other machine learning problem like regression, language generation, skill learning, etc.</p>

<p>The <b>key assumption</b> here is that meta-training tasks and meta-testing tasks are drawn i.i.d. from the same task 
distribution $\mathcal{T}_1, \cdots, \mathcal{T}_n,\mathcal{T}_\mathrm{test} \sim p(\mathcal{T})$, meaning that tasks 
must share structure.</p>

<p>Analogous to more data in machine learning, the more tasks, the better! You can say that meta learning is transfer 
learning with many source tasks.</p>

<p>The following is some terminology for different things you will hear when talking about meta learning:</p>

<ul>
  <li>The task-specific training set $\mathcal{D}_i^\mathrm{tr}$ is often referred to as the <em>support set</em> or the <em>context</em>.</li>
  <li>The task test dataset $\mathcal{D}_i^\mathrm{test}$ is called the <em>query set</em>.</li>
  <li><em>k-shot learning</em> refers to learning with <strong>k</strong> examples per class.</li>
</ul>

<h3 id="a-general-recipe-for-meta-learning-algorithms">A general recipe for meta learning algorithms</h3>

<p>Let’s formalize meta supervised learning in a mechanistic view. We are looking for a function 
$y^\mathrm{ts} = f_\theta(\mathcal{D}^\mathrm{tr}, x^\mathrm{ts})$, which is trained on the data $\{\mathcal{D}_i\}_{i=1,\cdots,n}$. 
This formulation reduces the meta-learning problem to the design and optimization of $f_\theta$.</p>

<p>To approach a problem using meta learning, you will need to decide on two steps:</p>

<ol>
  <li>What is my form of $f_\theta(\mathcal{D}^\mathrm{tr}, x^\mathrm{ts})$?</li>
  <li>How do I optimize the meta-parameters $\theta$ with respect to the maximum-likelihood objective using meta-training data.</li>
</ol>

<p>The following lectures will focus on core methods for meta learning and unsupervised pre-trained methods!</p>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning" /><category term="course" /><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two tasks to share a common structure, and start thinking about meta learning.]]></summary></entry><entry><title type="html">CS-330 Lecture 1: Multi-Task Learning</title><link href="http://localhost:4000/blog/2024/cs330-stanford-mtl/" rel="alternate" type="text/html" title="CS-330 Lecture 1: Multi-Task Learning" /><published>2024-03-02T00:00:00+01:00</published><updated>2024-03-02T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/cs330-stanford-mtl</id><content type="html" xml:base="http://localhost:4000/blog/2024/cs330-stanford-mtl/"><![CDATA[<p>The goal of this lecture is to understand the key design decisions when building multi-task learning systems. Since I am
still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or 
whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p>

<p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_multitask_transfer_2023.pdf">here</a>.</p>

<h2 id="problem-statement">Problem statement</h2>

<p>We will first establish some notation that will be used throughout the course. Let’s first introduce the single-task 
supervised learning problem.</p>

\[\min_\theta \mathcal{L}(\theta, \mathcal{D}), \quad \text{s.t.} \quad \mathcal{D} = \{(x,y)_k\}\;.\]

<p>Here, $\mathcal{L}$ is the loss function, $\theta$ are the model parameters and $\mathcal{D}$ is the dataset. A typical 
example of a loss function would be the negative log-likelihood function $\mathcal{L}(\theta, \mathcal{D}) = - \mathbb{E}\left[\log f_\theta(y\vert x)\right]$.</p>

<p>We can formally define a <strong>task</strong> as follows<strong>:</strong></p>

\[\mathcal{T}_i := \{p_i(x), p_i(y\vert x), \mathcal{L}_i\}\;.\]

<p>Here, $p_i(x)$ is the input data distribution, $p_i(y\vert x)$ is the distribution of the target variable(s), and 
$\mathcal{L}_i$ is a task-specific loss function (can of course be the same for different tasks). The corresponding 
task datasets are $\mathcal{D}_i^\mathrm{tr} := \mathcal{D}_i$ and $\mathcal{D}_i^\mathrm{test}$.</p>

<p>Some examples of tasks:</p>

<ul>
   <li>Multi-task classification ($\mathcal{L_i}$ the same for each task)</li>
   <ul>
      <li>Per-language handwriting recognition.</li>
      <li>Personalized spam filter.</li>
   </ul>
   <li>Multi-label learning ($\mathcal{L_i}$ and $p_i(x)$ the same for each task)</li>
   <ul>
      <li>Face attribute recognition.</li>
      <li>Scene understanding.
      <div class="col-sm-5 mt-3 mt-md-0"><figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/weighted_mtl_objective.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>
</div>
      </li>
   </ul>
</ul>

<p>It is important to realize that $\mathcal{L}_i$ might change across tasks, for example when mixing discrete from
continuous data or if there are multiple metrics that you care about.</p>

<hr />

<h2 id="models-objectives-optimization">Models, objectives, optimization</h2>

<p>One way of helping a model identify different tasks would be to condition the model function by a task descriptor 
$z_i$: $f_\theta(y\vert x, z_i)$. This could be anything ranging from user features, language descriptions, or formal 
task specifications. The next subsections will focus on how to condition the model, which objective should be used, and 
how the objective should be optimized.</p>

<h3 id="model">Model</h3>

<figure class="figure col-sm-10 float-right">
    <img src="/assets/img/blog/cs330/2/mult_gating.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Network architecture for task-specific independent subnetworks.</figcaption>
</figure>

<p>Let’s first think about how we can condition on the task in order to share <strong>as little information</strong> as possible. The
answer to this is simple: you can create a function that uses multiplicative gating with a one-hot encoding of the task
. The model function would be $f_\theta(y \vert x, z_i) = \sum_j \mathbb{1}(z_i=j)f_{\theta_i}(x)$. This results in
independent training with a single network per tasks; there are no shared parameters. This can be seen in the figure above.</p>

<p>On the other extreme, you could simply concatenate $z_i$ with the input and/or activations in the model. In this case, 
all parameters are shared (except the ones directly following $z_i$, in case it is one-hot).</p>

<p>This give rise to a question: can you phrase the multi-task learning objective parameters $\theta = \theta_\mathrm{sh} 
\cup \theta_i$, where $\theta_\mathrm{sh}$ are shared parameters and $\theta_i$ are task-specific parameters? Our 
objective function becomes the following:</p>

\[\min_{\theta_\mathrm{sh}, \theta_1, \cdots, \theta_T} \sum_{i=1}^T \mathcal{L}_i(\theta_\mathrm{sh} \cup \theta_i, \mathcal{D}_i)\;.\]

<p>In this case, choosing how to condition on $z_i$ is equivalent to choosing how and where to share model parameters. We 
will now look into some basic ways to condition a model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/concat_cond.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Concatenation-based conditioning.</figcaption>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/additive_cond.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Additive conditioning.</figcaption>
    </div>
</div>

<p><br />Can you see why additive conditioning in this way is equivalent to concatenation-based conditioning? Hint: think about 
how matrix multiplication splits the parameters when concatenating<d-footnote>You can find the solution to this question in the <a href="https://cs330.stanford.edu/materials/cs330_multitask_transfer_2023.pdf">lecture slides</a> (slide 13).</d-footnote>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/multi_head.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Multi-head architecture conditioning.</figcaption>
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/2/mult_cond.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <figcaption class="figure-caption text-center">Multiplicative conditioning.</figcaption>
    </div>
</div>
<p><br /></p>

<p>One benefit of multiplicative conditioning is that you have this multiplicative gating, allowing more expressiveness 
per layer. It generalizes independent networks and independent heads.</p>

<p>There are more complex conditioning techniques, and a lot of research has gone into this topic, such as Cross-Stitch Networks <d-cite key="misra2016cross"></d-cite>, 
Multi-Task Attention Network <d-cite key="liu2019end"></d-cite>, Deep Relation Networks <d-cite key="dai2017detecting"></d-cite>, 
Perceiver IO <d-cite key="jaegle2021perceiver"></d-cite>, and more.</p>

<p>Unfortunately, these design choices are <strong>problem dependent</strong>, largely guided by <strong>intuition</strong> or <strong>knowledge</strong> about 
the problem, and currently more of an <strong>art</strong> than a science.</p>

<h3 id="objectives">Objectives</h3>

<p>We already saw a previous example of a multi-task objective function. Let’s start with the vanilla multi-task learning 
(MTL) objective: $\min_\theta \sum_{i=1}^T \mathcal{L}_i(\theta, \mathcal{D_i})$. Let’s now show some other ways to 
construct multi-task objective functions.</p>

<ol>
  <li>
    <p>Weighted multi-task learning (manually based on priority or dynamically adjust weights throughout training):</p>

\[\min_\theta \sum_{i=1}^T w_i \mathcal{L}_i(\theta, \mathcal{D_i})\;.\]
  </li>
  <li>
    <p>Minimax multi-task learning to optimize for the worst-case task loss (useful in robustness or fairness):</p>

\[\min_\theta \max_i \mathcal{L}_i(\theta, \mathcal{D_i})\;.\]
  </li>
  <li>
    <p>You can use various <strong>heuristics</strong> to construct your objective function. One example is to encourage gradients to have similar magnitudes across tasks.</p>
  </li>
</ol>

<h3 id="optimization">Optimization</h3>

<p>For the vanilla MTL objective, a basic training approach follows the following steps:</p>

<ol>
  <li>Sample mini-batch of tasks $\mathcal{B} = {\mathcal{T}_i}$.</li>
  <li>Sample mini-batch of datapoints for each task $\mathcal{D}^b_i \sim \mathcal{D}_i$.</li>
  <li>Compute mini-batch loss $\hat{\mathcal{L}}(\theta, \mathcal{B}) = \sum_{\mathcal{T}_k \in \mathcal{B}} \mathcal{L}_k(\theta, \mathcal{D}_k^b)$.</li>
  <li>Backpropagate the loss to compute $\nabla_\theta \hat{\mathcal{L}}$.</li>
  <li>Perform a step of gradient descent with some optimizer.</li>
  <li>Repeat from step 1.</li>
</ol>

<p>This process ensures that tasks are sampled uniformly, regardless of data quantities. However, it is important to ensure 
that the task labels, and the loss function, are on the same scale.</p>

<hr />

<h2 id="challenges">Challenges</h2>

<p>There are multiple challenges that come with multi-task learning.</p>

<ol>
  <li>
    <p><strong>Negative transfer</strong>: Sometimes independent subnetworks work better than parameter sharing. This could be due to <strong>optimization challenges</strong> (cross-task interference or tasks learning at different rates), or <strong>limited representational capacity</strong> (multi-task networks often need to be <em>much larger</em> than their single-task counterparts).</p>

    <p>In the case of negative transfer, you should share less across tasks. You can also add a regularization term to the objective function, to allow <em>soft parameter sharing</em>:</p>

\[\min_{\theta_\mathrm{sh}, \theta_1, \cdots, \theta_T} \sum_{i=1}^T \mathcal{L}_i(\theta_\mathrm{sh} \cup \theta_i, \mathcal{D}_i) + \lambda \sum_{i^\prime = 1}^T \left\Vert \theta_i - \theta_i^\prime \right\Vert\;.\]

    <p>This allows for more fluid degrees of parameters sharing. However, it does add another set of hyperparameters, and it more memory intensive.</p>
  </li>
  <li><strong>Overfitting</strong>: You might not be sharing enough parameters. Since multi-task learning is equivalent to a form of regularization, the solution could be to share more parameters.</li>
  <li><strong>Having many tasks</strong>: You might wonder how to train all tasks together and which ones will be complementary. Unfortunately, no closed-form solution exists for measuring task similarity. Nevertheless, there are ways to approximate it from one training run <d-cite key="fifty2021efficiently"></d-cite> <d-cite key="xie2024doremi"></d-cite>.</li>
</ol>

<hr />

<h2 id="case-study-of-real-world-multi-task-learning">Case study of real-world multi-task learning</h2>

<p>In this case study, we will discuss the paper “Recommending What Video to Watch Next: A Multitask Ranking System” <d-cite key="zhao2019recommending"></d-cite>. They 
introduce a large scale multi-objective ranking system for recommending what video to watch next on an industrial video 
sharing platform. The system faces many real-world challenges, including the presence of multiple competing ranking 
objectives, as well as implicit selection biases in user feedback.</p>

<p>The framework is constructed as follows:</p>

<ul>
  <li><strong>Inputs</strong>: What the user is currently watching (query video) and user features</li>
</ul>

<p>The procedure is the following:</p>

<ol>
  <li>Generate a few hundred of <strong>candidate videos</strong> (by pooling videos from multiple candidate generation algorithms such as matching topics of the query video, videos frequently watched with the query video, and others).</li>
  <li><strong>Rank</strong> the candidates.</li>
  <li><strong>Serve</strong> the top ranking videos to the user.</li>
</ol>

<p>The central topic of this paper is the ranking system. The authors decide that the inputs to the ranking model are the 
<strong>query video</strong>, <strong>candidate video</strong>, and <strong>context features</strong>. The model attempts to output a weighted combination of 
<strong>engagement</strong> and <strong>satisfaction</strong> predictions, which results in a ranking score. The score weights are manually tuned.</p>

<div>
<figure class="figure col-sm-7 float-right">
    <img src="/assets/img/blog/cs330/2/expert_model.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Multi-gate Mixture-of-Expert architecture.</figcaption>
</figure>

<p>On choice for the model architecture is a “shared-bottom model”, which has some shared bottom layers which split into
separate heads for each task. However, this will harm learning when the correlation between tasks is low. Instead, they
opt for a form of soft-parameter sharing that they call <b>Multi-gate Mixture-of-Experts</b> (MMoE). As you can see in the
figure, this architecture allows different parts of the network to “specialize” in certain tasks as experts. For each
task, an attention-like score is computed that decides which combination of experts should be used.</p>
</div>

<p>Formally, let’s call the expert networks $f_i(x)$. We then decide which expert to use for input $x$ and task $k$ by 
computing $g^k(x) = \mathrm{softmax}(W_{g^k}x)$. The features are then computed from the selected experts as 
$f_k(x) = \sum_{i=1}^n g_{(i)}^k(x)f_i(x)$. The output can finally be denoted by $y_k = h^k(f^k(x))$.</p>

<p>In the paper, they trained them model in temporal order, running training continuously to consume newly arriving data. 
They perform online A/B testing in comparison to the production system based on some live metrics, and stress that
model <strong>computational efficiency matters</strong>.</p>

<div>
<figure class="figure col-sm-7 float-right">
    <img src="/assets/img/blog/cs330/2/paper_results.png" class="img-fluid" alt="Alt text." />
    <figcaption class="figure-caption text-center">Results from different model configurations.</figcaption>
</figure>

<p>From the results, you can see that this sort of architecture definitely helps. Furthermore, they found that there was a 
20% change of <b>gating polarization</b> during distributed training. This means that not all experts are utilized equally
and there is a bias to some expert(s). They utilized drop-out on the experts to counteract this problem.</p>
</div>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning" /><category term="course" /><summary type="html"><![CDATA[This is the first lecture of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the key design decisions when building multi-task learning systems.]]></summary></entry><entry><title type="html">CS-330: Deep Multi-Task and Meta Learning - Introduction</title><link href="http://localhost:4000/blog/2024/cs330-stanford-introduction/" rel="alternate" type="text/html" title="CS-330: Deep Multi-Task and Meta Learning - Introduction" /><published>2024-03-01T00:00:00+01:00</published><updated>2024-03-01T00:00:00+01:00</updated><id>http://localhost:4000/blog/2024/cs330-stanford-introduction</id><content type="html" xml:base="http://localhost:4000/blog/2024/cs330-stanford-introduction/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>The course <a href="https://cs330.stanford.edu/">CS 330: Deep Multi-Task and Meta Learning</a>, by <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, is taught
on a yearly basis and discusses the foundations and current state of multi-task learning and meta learning.</p>

<p><strong>:warning: Note:</strong> I am discussing the content of the edition in Fall 2023, which no longer includes reinforcement learning.
If you are interested in this, I will be auditing <a href="https://cs224r.stanford.edu/">CS 224R Deep Reinforcement Learning</a>
later this spring, which is also taught by <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>.</p>

<p>In an attempt to improve my writing skills and provide useful summaries/voice my opinions, I have decided to discuss 
the content of every lecture in this blog. In this post, I will give an overview of the course and why it is important 
for AI, especially now.</p>

<p>This course will focus on solving problems that are composed of multiple tasks, and studies how structure that arises from these multiple tasks can be leveraged to learn more efficiently/effectively, including:</p>

<ul>
  <li>Self-supervised pre-training for downstream few-shot learning and transfer learning.</li>
  <li>Meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly.</li>
  <li>Curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer.</li>
</ul>

<hr />

<h2 id="lectures">Lectures</h2>

<p>The lecture schedule of the course is as follows:</p>
<ol>
  <li><a href="/blog/2024/cs330-stanford-mtl/">Multi-task learning</a></li>
  <li><a href="/blog/2024/cs330-stanford-tl-ml/">Transfer learning &amp; meta learning</a></li>
  <li><a href="/blog/2024/cs330-stanford-bbml-icl/">Black-box meta-learning &amp; in-context learning</a></li>
  <li><a href="/blog/2024/cs330-stanford-obml/">Optimization-based meta-learning</a></li>
  <li><a href="/blog/2024/cs330-stanford-fsl-ml/">Few-shot learning via metric learning</a></li>
  <li><a href="/blog/2024/cs330-stanford-upt-fsl-cl/">Unsupervised pre-training for few-shot learning (contrastive)</a></li>
  <li>Unsupervised pre-training for few-shot learning (generative)</li>
  <li>Advanced meta-learning topics (task construction)</li>
  <li>Variational inference</li>
  <li>Bayesian meta-learning</li>
  <li>Advanced meta-learning topics (large-scale meta-optimization)</li>
  <li>Lifelong learning</li>
  <li>Domain Adaptation and Domain Generalization</li>
  <li>Frontiers &amp; Open Challenges</li>
</ol>

<p>I am excited to start discussing these topics in greater detail! Check this page regularly for updates, since I will 
link to new posts whenever they are available!</p>

<hr />

<h2 id="why-multi-task-and-meta-learning">Why multi-task and meta-learning?</h2>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/cs330/1/robotics_example.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Robots are embodied in the real world, and must generalize across tasks. In order to do so, they need some common sense 
understanding and supervision can’t be taken for granted.</p>

<p>Earlier robotics and reinforcement research mainly focused on problems that required learning a task from scratch. This 
problem is even present in other fields, such as object detection or speech recognition. However, as opposed to these 
problems, <strong>humans are generalists</strong> that exploit common structures to solve new problems more efficiently.</p>

<p>Going beyond the case of generalist agents, deep multi-task and meta learning useful for any problems where a <strong>common 
structure</strong> can benefit the efficiency or effectiveness of a model. It can be impractical to develop models for each
specific task (e.g. each robot, person, or disease), especially if the data that you have access to for these individual
tasks is <strong>scarce</strong>.</p>

<p>If you need to <strong>quickly learn something new</strong>, you need to utilize prior experiences (e.g. few-shot learning) to make 
decisions.</p>

<p>But why now? Right now, with the speed of research advancements in AI, many researchers are looking into utilizing 
multi-model information to develop their models. Especially in robotics, foundation models seem <strong>the</strong> topic in 2024,
and many advancements have been made in the past year <d-cite key="zhao2023learning"></d-cite>, <d-cite key="open_x_embodiment_rt_x_2023"></d-cite>, <d-cite key="octo_2023"></d-cite>, <d-cite key="brohan2023rt"></d-cite>.</p>

<hr />

<h2 id="what-are-tasks">What are tasks?</h2>

<p>Given a dataset $\mathcal{D}$ and loss function $\mathcal{L}$, we hope to develop a model $f_\theta$. Different tasks 
can be used to train this model, with some simple examples being objects, people, objectives, lighting conditions, 
words, languages, etc.</p>

<p>The <strong>critical assumption</strong> here is that different tasks must share some common structure. However, in practice, this 
is very often the case, even for tasks that seem unrelated. For example the laws of physics and the rules of English
can be shared among many tasks.</p>

<ol>
  <li>The multi-task problem: Learn <strong>a set of tasks</strong> more quickly or more proficiently than learning them independently.</li>
  <li>Given data on previous task(s), learn <strong>a new task</strong> more quickly and/or more proficiently.</li>
</ol>

<blockquote>
  <p>Doesn’t multi-task learning reduce to single-task learning?</p>
</blockquote>

<p>This is indeed the case when aggregating data across multiple tasks, which is actually one approach to multi-task 
learning. However, what if you want to learn new tasks? And how do you tell the model which task to do? And what if 
aggregating doesn’t work?</p>

<hr />]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning" /><category term="course" /><summary type="html"><![CDATA[I have been incredibly interested in the recent wave of multimodal foundation models, especially in robotics and sequential decision-making. Since I never had a formal introduction to this topic, I decided to audit the Deep Multi-Task and Meta Learning course, which is taught yearly by Chelsea Finn at Stanford. I will mainly document my takes on the lectures, hopefully making it a nice read for people who would like to learn more about this topic!]]></summary></entry></feed>